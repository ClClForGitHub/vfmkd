# CLIPæ¨¡å‹ä½¿ç”¨è¯´æ˜

## ğŸ“‹ æ¨¡å‹ä¿¡æ¯

### æ¨¡å‹ç±»å‹
- **æ¨¡å‹æ¶æ„**: `ViT-B-32` (Vision Transformer Base, 32x32 patch size)
- **é¢„è®­ç»ƒæ•°æ®é›†**: `laion2b_s34b_b79k` (LAION-2Bæ•°æ®é›†)
- **æƒé‡æ¥æº**: OpenCLIP (https://github.com/mlfoundations/open_clip)
- **æƒé‡æ ¼å¼**: 
  - `open_clip_model.safetensors` (ä¼˜å…ˆ)
  - `open_clip_pytorch_model.bin` (å¤‡é€‰)

### æƒé‡è·¯å¾„
```
/home/team/zouzhiyuan/vfmkd/weights/clip/ViT-B-32-laion2B-s34B-b79K/
â”œâ”€â”€ open_clip_model.safetensors
â””â”€â”€ open_clip_pytorch_model.bin
```

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### 1. æ¨¡å‹åŠ è½½

```python
import open_clip
import torch
from safetensors.torch import load_file as load_safetensor
from pathlib import Path

# è®¾å¤‡é…ç½®ï¼ˆé»˜è®¤ä½¿ç”¨cuda:4ï¼‰
device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')

# æƒé‡è·¯å¾„
weight_dir = Path("weights/clip/ViT-B-32-laion2B-s34B-b79K")
weight_path = weight_dir / "open_clip_model.safetensors"  # æˆ– open_clip_pytorch_model.bin

# åˆ›å»ºæ¨¡å‹ï¼ˆä¸ä½¿ç”¨pretrainedï¼Œé¿å…ç½‘ç»œè¯·æ±‚ï¼‰
model, _, preprocess = open_clip.create_model_and_transforms(
    'ViT-B-32',
    pretrained=None,  # å…³é”®ï¼šä¸ä½¿ç”¨pretrainedï¼Œé¿å…ç½‘ç»œè¯·æ±‚
    device=device
)

# åŠ è½½æœ¬åœ°æƒé‡
if weight_path.suffix == '.safetensors':
    state_dict = load_safetensor(str(weight_path))
else:
    state_dict = torch.load(str(weight_path), map_location=device)

model.load_state_dict(state_dict, strict=False)
model.eval()
```

### 2. æ–‡æœ¬ç¼–ç 

```python
# è·å–tokenizer
tokenizer = open_clip.get_tokenizer('ViT-B-32')

# å®šä¹‰æç¤ºè¯
positive_prompts = [
    'a person', 'a man', 'a woman', 'a child', 'a group of people',
    'a bicycle', 'a car', 'a motorcycle', 'a bus', 'a truck',
    'a building', 'a room', 'an object'
    # ... å®Œæ•´çš„COCO 80ç±»åˆ«
]

negative_prompts = [
    'sky', 'ground', 'road', 'wall', 'grass', 'tree', 'background',
    'a sleeve', 'a pant leg', 'clothing', 'a texture', 'a part of', 'a fragment'
    # ... æ›´å¤šèƒŒæ™¯/ç‰‡æ®µæç¤ºè¯
]

# ç¼–ç æ–‡æœ¬ç‰¹å¾
with torch.no_grad():
    tokens_pos = tokenizer(positive_prompts).to(device)
    tokens_neg = tokenizer(negative_prompts).to(device)
    
    text_features_pos = model.encode_text(tokens_pos)
    text_features_pos = text_features_pos / text_features_pos.norm(dim=-1, keepdim=True)  # L2å½’ä¸€åŒ–
    
    text_features_neg = model.encode_text(tokens_neg)
    text_features_neg = text_features_neg / text_features_neg.norm(dim=-1, keepdim=True)  # L2å½’ä¸€åŒ–
```

### 3. å›¾åƒé¢„å¤„ç†

```python
from PIL import Image
import cv2
import numpy as np

# æ–¹æ³•1: åŸå§‹æ–¹æ³•ï¼ˆæ©ç å¤–ç½®ç°128ï¼Œæ©ç å†…Ã—1.05ï¼‰
def preprocess_original(cropped_pil: Image.Image, mask_resized: np.ndarray) -> Image.Image:
    cropped_np = np.array(cropped_pil).astype(np.float32)
    background_value = 128.0
    mask_zero = mask_resized == 0
    cropped_np[mask_zero] = background_value
    
    mask_positive = mask_resized > 0
    if mask_positive.any():
        cropped_np[mask_positive] *= 1.05
    cropped_np = np.clip(cropped_np, 0.0, 255.0)
    return Image.fromarray(cropped_np.astype(np.uint8))

# æ–¹æ³•2: åŸå§‹è£å‰ªï¼ˆä¸å¤„ç†ï¼‰
def preprocess_raw_crop(cropped_pil: Image.Image, mask_resized: np.ndarray) -> Image.Image:
    return cropped_pil

# æ–¹æ³•3: è½¯é®ç½©ï¼ˆalpha blendingï¼‰
def preprocess_soft_mask(cropped_pil: Image.Image, mask_resized: np.ndarray, alpha: float = 0.4) -> Image.Image:
    cropped_np = np.array(cropped_pil).astype(np.float32)
    background_value = 128.0
    background = np.full_like(cropped_np, background_value)
    
    mask_positive = mask_resized > 0.5
    mask_negative = ~mask_positive
    
    # æ©ç å¤–åŒºåŸŸï¼šåŸå›¾ * (1-alpha) + ç°è‰² * alpha
    cropped_np[mask_negative] = (
        cropped_np[mask_negative] * (1 - alpha) + 
        background[mask_negative] * alpha
    )
    
    # æ©ç å†…åŒºåŸŸï¼šç¨å¾®å¢å¼º
    if mask_positive.any():
        cropped_np[mask_positive] *= 1.05
    
    cropped_np = np.clip(cropped_np, 0.0, 255.0)
    return Image.fromarray(cropped_np.astype(np.uint8))

# ä½¿ç”¨preprocesså‡½æ•°è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
processed_pil = preprocess_original(cropped_pil, mask_resized)
image_tensor = preprocess(processed_pil).unsqueeze(0).to(device)  # [1, 3, H, W]
```

### 4. å›¾åƒç¼–ç ä¸ç›¸ä¼¼åº¦è®¡ç®—

```python
# æ‰¹é‡å¤„ç†å›¾åƒ
image_tensors = [preprocess(img).to(device) for img in processed_images]
image_batch_tensor = torch.stack(image_tensors, dim=0)  # [N, 3, H, W]

# ç¼–ç å›¾åƒç‰¹å¾
with torch.no_grad():
    image_features = model.encode_image(image_batch_tensor)
    image_features = image_features / image_features.norm(dim=-1, keepdim=True)  # L2å½’ä¸€åŒ–
    
    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ = å½’ä¸€åŒ–ç‰¹å¾çš„ç‚¹ç§¯ï¼‰
    sim_pos = image_features @ text_features_pos.T  # [N, num_positive_prompts]
    sim_neg = image_features @ text_features_neg.T  # [N, num_negative_prompts]
    
    # æ‰¾åˆ°æœ€é«˜åˆ†
    pos_scores, pos_idxs = sim_pos.max(dim=1)  # [N], [N]
    neg_scores, neg_idxs = sim_neg.max(dim=1)  # [N], [N]
```

## ğŸ“Š è¿”å›å‚æ•°è¯´æ˜

### 1. `model.encode_image(image_tensor)` è¿”å›å€¼

**è¾“å…¥**:
- `image_tensor`: `torch.Tensor`, shape `[N, 3, H, W]` æˆ– `[3, H, W]`
  - ç»è¿‡`preprocess`å‡½æ•°å¤„ç†åçš„å›¾åƒå¼ é‡
  - å€¼åŸŸ: å·²å½’ä¸€åŒ–ï¼ˆImageNetæ ‡å‡†åŒ–ï¼‰

**è¾“å‡º**:
- `image_features`: `torch.Tensor`, shape `[N, 512]` æˆ– `[512]`
  - å›¾åƒç‰¹å¾å‘é‡ï¼ˆæœªå½’ä¸€åŒ–ï¼‰
  - ç»´åº¦: 512 (ViT-B-32çš„embeddingç»´åº¦)

**ç¤ºä¾‹**:
```python
image_features = model.encode_image(image_tensor)
print(f"Shape: {image_features.shape}")  # [N, 512]
print(f"Mean: {image_features.mean():.4f}, Std: {image_features.std():.4f}")
```

### 2. `model.encode_text(text_tokens)` è¿”å›å€¼

**è¾“å…¥**:
- `text_tokens`: `torch.Tensor`, shape `[N, seq_len]`
  - ç»è¿‡tokenizerç¼–ç çš„æ–‡æœ¬tokenåºåˆ—
  - é€šå¸¸`seq_len=77`ï¼ˆCLIPçš„æœ€å¤§åºåˆ—é•¿åº¦ï¼‰

**è¾“å‡º**:
- `text_features`: `torch.Tensor`, shape `[N, 512]`
  - æ–‡æœ¬ç‰¹å¾å‘é‡ï¼ˆæœªå½’ä¸€åŒ–ï¼‰
  - ç»´åº¦: 512 (ä¸å›¾åƒç‰¹å¾ç»´åº¦ç›¸åŒ)

**ç¤ºä¾‹**:
```python
tokens = tokenizer(["a person", "a car"])
text_features = model.encode_text(tokens)
print(f"Shape: {text_features.shape}")  # [2, 512]
```

### 3. åŸç”ŸCLIPåˆ†ç±»ï¼ˆæ¨èæ–¹å¼ï¼‰

**ä½¿ç”¨ logit_scale + softmax å¾—åˆ°æ¦‚ç‡**:
```python
# å½’ä¸€åŒ–ç‰¹å¾
image_features_norm = image_features / image_features.norm(dim=-1, keepdim=True)
text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)

# è®¡ç®— logitsï¼ˆåŸç”Ÿ CLIP æ–¹å¼ï¼‰
logit_scale = model.logit_scale.exp()  # æ¸©åº¦å‚æ•°ï¼ˆé€šå¸¸å‡ ååˆ°ä¸€ç™¾å¤šï¼‰
logits = logit_scale * image_features_norm @ text_features_norm.T  # [N_images, N_texts]

# softmax å¾—åˆ°æ¦‚ç‡
probs = logits.softmax(dim=-1)  # [N_images, N_texts]ï¼Œæ¯è¡Œå’Œä¸º1.0
```

**æ¦‚ç‡èŒƒå›´**:
- ç†è®ºä¸Š: `[0.0, 1.0]` (softmaxåçš„æ¦‚ç‡)
- æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡å’Œä¸º 1.0
- é«˜ç½®ä¿¡åº¦: æœ€é«˜æ¦‚ç‡ `> 0.5` é€šå¸¸è¡¨ç¤ºæ¨¡å‹å¾ˆç¡®å®š

**ä¸ºä»€ä¹ˆç”¨æ¦‚ç‡è€Œä¸æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼Ÿ**
- æ¦‚ç‡æœ‰æ˜ç¡®çš„è¯­ä¹‰ï¼šè¡¨ç¤º"è¿™ä¸ªå›¾åƒå±äºæŸä¸ªç±»åˆ«çš„å¯èƒ½æ€§"
- æ¦‚ç‡å€¼åœ¨ [0, 1] èŒƒå›´å†…ï¼Œæ›´å®¹æ˜“ç†è§£å’Œè®¾ç½®é˜ˆå€¼
- ç¬¦åˆCLIPåŸç”Ÿçš„zero-shotåˆ†ç±»æµç¨‹

### 4. ç»„ä»¶æ‰“åˆ†ç»“æœ (`comp` å­—å…¸)

åœ¨`test_bbox_strategies.py`ä¸­ï¼Œæ¯ä¸ªç»„ä»¶ä¼šæ·»åŠ ä»¥ä¸‹CLIPç›¸å…³å­—æ®µ:

```python
comp = {
    # ... å…¶ä»–å­—æ®µ ...
    
    # CLIPè¯­ä¹‰æ‰“åˆ†ï¼ˆä½¿ç”¨åŸç”Ÿæ¦‚ç‡ï¼‰
    's_pos': float,              # æ­£ç±»æœ€é«˜æ¦‚ç‡ (0.0-1.0)
    's_neg': float,              # è´Ÿç±»æœ€é«˜æ¦‚ç‡ (0.0-1.0)
    's_pos_text': str,          # åŒ¹é…çš„æ­£ç±»æç¤ºè¯ (å¦‚ "a person")
    's_neg_text': str,           # åŒ¹é…çš„è´Ÿç±»æç¤ºè¯ (å¦‚ "ground")
    'semantic_multiplier': float, # è¯­ä¹‰ä¹˜æ•° (2.0 / 1.0 / 0.5 / 0.1)
}
```

**`semantic_multiplier` è§„åˆ™**ï¼ˆåŸºäºæ¦‚ç‡æ€»å’Œï¼‰:
- `2.0`: `p_pos_sum > 0.7` ä¸” `p_neg_sum < 0.2` (å¾ˆç¡®å®šæ˜¯å‰æ™¯ç›®æ ‡)
- `1.0`: `p_pos_sum > 0.4` (ä¸­ç­‰ç½®ä¿¡åº¦æ­£ç±»)
- `0.5`: å…¶ä»–æƒ…å†µï¼ˆæ¨¡ç³Š/ä¸ç¡®å®šï¼‰
- `0.1`: `p_neg_sum > 0.5` (æ˜æ˜¾æ˜¯èƒŒæ™¯/ç¢ç‰‡)

**æ¦‚ç‡ç»Ÿè®¡é‡**:
- `p_pos_sum`: æ‰€æœ‰æ­£ç±»æç¤ºè¯çš„æ¦‚ç‡æ€»å’Œ
- `p_neg_sum`: æ‰€æœ‰è´Ÿç±»æç¤ºè¯çš„æ¦‚ç‡æ€»å’Œ
- `p_pos_max`: æ­£ç±»ä¸­æœ€é«˜æ¦‚ç‡
- `p_neg_max`: è´Ÿç±»ä¸­æœ€é«˜æ¦‚ç‡

## ğŸ“ å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ï¼ˆåŸç”ŸCLIPæ–¹å¼ï¼‰

```python
import torch
import open_clip
from PIL import Image
from pathlib import Path
from safetensors.torch import load_file as load_safetensor

# 1. åŠ è½½æ¨¡å‹
device = torch.device('cuda:4')
weight_path = Path("weights/clip/ViT-B-32-laion2B-s34B-b79K/open_clip_model.safetensors")

model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained=None, device=device)
state_dict = load_safetensor(str(weight_path))
model.load_state_dict(state_dict, strict=False)
model.eval()

# 2. å‡†å¤‡æ–‡æœ¬ç‰¹å¾
tokenizer = open_clip.get_tokenizer('ViT-B-32')
positive_prompts = ['a person', 'a car', 'a building']
negative_prompts = ['sky', 'ground', 'background']
all_prompts = positive_prompts + negative_prompts
num_pos = len(positive_prompts)

tokens = tokenizer(all_prompts).to(device)
with torch.no_grad():
    text_features = model.encode_text(tokens)
    text_features = text_features / text_features.norm(dim=-1, keepdim=True)

# 3. å¤„ç†å›¾åƒ
image_pil = Image.open("image.jpg")
image_tensor = preprocess(image_pil).unsqueeze(0).to(device)

# 4. ç¼–ç å›¾åƒå¹¶ä½¿ç”¨åŸç”ŸCLIPæ–¹å¼è®¡ç®—æ¦‚ç‡
with torch.no_grad():
    # ç¼–ç å›¾åƒ
    image_features = model.encode_image(image_tensor)
    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
    
    # è®¡ç®— logitsï¼ˆåŸç”Ÿ CLIP æ–¹å¼ï¼‰
    logit_scale = model.logit_scale.exp()
    logits = logit_scale * image_features @ text_features.T  # [1, num_prompts]
    
    # softmax å¾—åˆ°æ¦‚ç‡
    probs = logits.softmax(dim=-1)  # [1, num_prompts]
    probs = probs[0]  # [num_prompts]
    
    # åˆ†ç¦»æ­£ç±»å’Œè´Ÿç±»æ¦‚ç‡
    pos_probs = probs[:num_pos]
    neg_probs = probs[num_pos:]
    
    # è®¡ç®—ç»Ÿè®¡é‡
    p_pos_sum = pos_probs.sum().item()
    p_neg_sum = neg_probs.sum().item()
    p_pos_max, pos_max_idx = pos_probs.max(dim=0)
    p_neg_max, neg_max_idx = neg_probs.max(dim=0)
    top_prob, top_idx = probs.max(dim=0)
    
    print(f"æœ€é«˜æ¦‚ç‡: {top_prob.item():.4f}")
    print(f"åŒ¹é…æç¤ºè¯: {all_prompts[top_idx.item()]}")
    print(f"æ­£ç±»æ¦‚ç‡æ€»å’Œ: {p_pos_sum:.4f}")
    print(f"è´Ÿç±»æ¦‚ç‡æ€»å’Œ: {p_neg_sum:.4f}")
    print(f"æ­£ç±»æœ€é«˜æ¦‚ç‡: {p_pos_max.item():.4f} ({positive_prompts[pos_max_idx.item()]})")
    print(f"è´Ÿç±»æœ€é«˜æ¦‚ç‡: {p_neg_max.item():.4f} ({negative_prompts[neg_max_idx.item()]})")
    
    # æ ¹æ®æ¦‚ç‡è®¾è®¡è¯­ä¹‰ä¹˜æ•°
    if p_neg_sum > 0.5:
        semantic_multiplier = 0.1
    elif p_pos_sum > 0.7 and p_neg_sum < 0.2:
        semantic_multiplier = 2.0
    elif p_pos_sum > 0.4:
        semantic_multiplier = 1.0
    else:
        semantic_multiplier = 0.5
    
    print(f"è¯­ä¹‰ä¹˜æ•°: {semantic_multiplier}")
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æƒé‡åŠ è½½**: å¿…é¡»ä½¿ç”¨`pretrained=None`åˆ›å»ºæ¨¡å‹ï¼Œç„¶åæ‰‹åŠ¨åŠ è½½æœ¬åœ°æƒé‡ï¼Œé¿å…ç½‘ç»œè¯·æ±‚
2. **ç‰¹å¾å½’ä¸€åŒ–**: è®¡ç®—logitså‰å¿…é¡»å¯¹å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾è¿›è¡ŒL2å½’ä¸€åŒ–
3. **ä½¿ç”¨logit_scale**: **å¿…é¡»**ä½¿ç”¨`model.logit_scale.exp()`ä½œä¸ºæ¸©åº¦å‚æ•°ï¼Œè¿™æ˜¯CLIPåŸç”Ÿæ–¹å¼çš„å…³é”®
4. **ä½¿ç”¨softmax**: å¿…é¡»å¯¹logitsåšsoftmaxå¾—åˆ°æ¦‚ç‡ï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
5. **æ‰¹å¤„ç†**: `encode_image`å’Œ`encode_text`éƒ½æ”¯æŒæ‰¹å¤„ç†ï¼Œå¯ä»¥æé«˜æ•ˆç‡
6. **è®¾å¤‡ä¸€è‡´æ€§**: ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼ˆé€šå¸¸æ˜¯GPUï¼‰
7. **æ¨ç†æ¨¡å¼**: ä½¿ç”¨`model.eval()`å’Œ`torch.no_grad()`ç¡®ä¿æ¨ç†æ•ˆç‡

## ğŸ”„ ä»ä½™å¼¦ç›¸ä¼¼åº¦è¿ç§»åˆ°åŸç”ŸCLIPæ–¹å¼

**æ—§æ–¹å¼ï¼ˆä¸æ¨èï¼‰**:
```python
similarity = image_features @ text_features.T  # ä½™å¼¦ç›¸ä¼¼åº¦ [-1, 1]
scores, indices = similarity.max(dim=1)
```

**æ–°æ–¹å¼ï¼ˆæ¨èï¼‰**:
```python
logit_scale = model.logit_scale.exp()  # æ¸©åº¦å‚æ•°
logits = logit_scale * image_features @ text_features.T  # logits
probs = logits.softmax(dim=-1)  # æ¦‚ç‡ [0, 1]
scores, indices = probs.max(dim=1)
```

**ä¼˜åŠ¿**:
- æ¦‚ç‡å€¼æœ‰æ˜ç¡®çš„è¯­ä¹‰ï¼ˆå±äºæŸä¸ªç±»åˆ«çš„å¯èƒ½æ€§ï¼‰
- æ‰€æœ‰ç±»åˆ«æ¦‚ç‡å’Œä¸º1.0ï¼Œæ›´å®¹æ˜“ç†è§£å’Œè®¾ç½®é˜ˆå€¼
- ç¬¦åˆCLIPåŸç”Ÿçš„zero-shotåˆ†ç±»æµç¨‹

## ğŸ”— ç›¸å…³æ–‡ä»¶

- ä¸»ä½¿ç”¨æ–‡ä»¶: `tools/core/bbox/test_bbox_strategies.py`
- æµ‹è¯•è„šæœ¬: `tools/core/bbox/test_clip_preprocessing.py`
- æƒé‡ç›®å½•: `weights/clip/ViT-B-32-laion2B-s34B-b79K/`

