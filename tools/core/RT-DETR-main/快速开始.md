# RT-DETRv2 快速开始指南

## 一、权重转换（必须步骤）

### 转换backbone权重

```bash
cd /home/team/zouzhiyuan/vfmkd/tools/core/RT-DETR-main

python convert_backbone_weights.py \
    --input /home/team/zouzhiyuan/vfmkd/outputs/distill_single_test_FGD/20251125_204817_rtdetrv2_edge_boost_fgd_gpu4_edge_boost_rtdetrv2/models/best_backbone_mmdet.pth \
    --output ./rtdetrv2_backbone_weights.pth
```

**说明**：
- 输入权重键名格式：`stem.stem1.conv.weight`
- 输出权重键名格式：`backbone.stem.stem1.conv.weight`
- RT-DETRv2模型中的backbone是作为子模块，所以需要`backbone.`前缀

## 二、训练RT-DETRv2

### 1. 准备配置文件

使用HGNetv2-L配置：
```bash
cd RT-DETR-main/rtdetrv2_pytorch
# 配置文件：configs/rtdetrv2/rtdetrv2_hgnetv2_l_6x_coco.yml
```

**重要**：在配置文件中设置 `pretrained: False`，因为我们将通过`-t`参数加载权重。

### 2. 单GPU训练

```bash
CUDA_VISIBLE_DEVICES=0 python tools/train.py \
    -c configs/rtdetrv2/rtdetrv2_hgnetv2_l_6x_coco.yml \
    -t ../../rtdetrv2_backbone_weights.pth \
    --use-amp \
    --seed=0
```

### 3. 多GPU训练

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun \
    --master_port=9909 \
    --nproc_per_node=4 \
    tools/train.py \
    -c configs/rtdetrv2/rtdetrv2_hgnetv2_l_6x_coco.yml \
    -t ../../rtdetrv2_backbone_weights.pth \
    --use-amp \
    --seed=0 \
    &> log.txt 2>&1 &
```

## 三、关键参数说明

| 参数 | 说明 | 示例 |
|------|------|------|
| `-c, --config` | 配置文件路径 | `-c configs/rtdetrv2/rtdetrv2_hgnetv2_l_6x_coco.yml` |
| `-t, --tuning` | **加载backbone权重**（只加载匹配的权重） | `-t ./rtdetrv2_backbone_weights.pth` |
| `-r, --resume` | 恢复训练（加载完整checkpoint） | `-r ./checkpoint.pth` |
| `--use-amp` | 启用混合精度训练 | `--use-amp` |
| `--test-only` | 仅测试模式 | `--test-only` |

**注意**：`-t`和`-r`不能同时使用。

## 四、权重加载机制

### load_tuning_state 工作流程

1. **加载checkpoint**
   ```python
   state = torch.load(path, map_location='cpu')
   ```

2. **选择权重源**
   - 如果有`ema`键：使用`state['ema']['module']`
   - 否则：使用`state['model']`

3. **键名匹配**
   - 遍历当前模型的state_dict
   - 检查checkpoint中是否有对应键
   - 检查形状是否匹配
   - 只加载匹配的权重（`strict=False`）

4. **结果**
   - ✅ 加载backbone权重（键名匹配`backbone.*`）
   - ✅ encoder和decoder保持随机初始化
   - ✅ 忽略不匹配的键（不会报错）

### 训练日志示例

```
tuning checkpoint from ./rtdetrv2_backbone_weights.pth
Load model.state_dict, {'missed': [...], 'unmatched': [...]}
```

- `missed`: 当前模型有但checkpoint没有的键（通常是encoder/decoder，正常）
- `unmatched`: 键名匹配但形状不匹配的键（会跳过）

## 五、完整示例

```bash
# 步骤1: 转换权重
cd /home/team/zouzhiyuan/vfmkd/tools/core/RT-DETR-main
python convert_backbone_weights.py \
    --input /home/team/zouzhiyuan/vfmkd/outputs/distill_single_test_FGD/20251125_204817_rtdetrv2_edge_boost_fgd_gpu4_edge_boost_rtdetrv2/models/best_backbone_mmdet.pth \
    --output ./rtdetrv2_backbone_weights.pth

# 步骤2: 训练模型
cd RT-DETR-main/rtdetrv2_pytorch
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun \
    --master_port=9909 \
    --nproc_per_node=4 \
    tools/train.py \
    -c configs/rtdetrv2/rtdetrv2_hgnetv2_l_6x_coco.yml \
    -t ../../rtdetrv2_backbone_weights.pth \
    --use-amp \
    --seed=0 \
    --output-dir ./output/rtdetrv2_hgnetv2_l_custom_backbone
```

## 六、常见问题

### Q: 如何确认权重加载成功？

查看训练日志，应该看到：
```
tuning checkpoint from ...
Load model.state_dict, ...
```

如果`missed`列表包含encoder/decoder相关键，这是正常的。

### Q: 如何冻结backbone？

在配置文件中：
```yaml
HGNetv2:
  freeze_at: 0      # 不冻结
  freeze_norm: True # 冻结BN层
```

或在optimizer中设置学习率为0：
```yaml
optimizer:
  params:
    - params: '^(?=.*backbone).*$'
      lr: 0.0
```

### Q: 权重格式不对怎么办？

检查权重文件结构：
```python
import torch
ckpt = torch.load('weights.pth', map_location='cpu')
print(ckpt.keys())  # 应该包含 'model' 或 'ema'
print(list(ckpt['model'].keys())[:10])  # 检查键名格式
```

## 七、相关文件

- **权重转换脚本**: `convert_backbone_weights.py`
- **详细文档**: `RT-DETRv2_使用说明.md`
- **训练脚本**: `RT-DETR-main/rtdetrv2_pytorch/tools/train.py`
- **权重加载逻辑**: `RT-DETR-main/rtdetrv2_pytorch/src/solver/_solver.py::load_tuning_state`
- **配置文件**: `RT-DETR-main/rtdetrv2_pytorch/configs/rtdetrv2/rtdetrv2_hgnetv2_l_6x_coco.yml`

