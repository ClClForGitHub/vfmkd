#!/usr/bin/env python3
"""
SA-1B Extract V2 (Hybrid Storage + Dynamic Protocol)

åŠŸèƒ½ï¼š
1. æ··åˆå­˜å‚¨: Images (å˜é•¿JPG + ç´¢å¼•è¡¨) + Features (å®šé•¿Float32)
2. åŠ¨æ€åè®®: è¿è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹ Teacher ç»´åº¦å’Œæ­¥é•¿ï¼Œç”Ÿæˆ config.json
3. é¢„å¤„ç†å¯¹é½: å­˜å‚¨ Resize åŽçš„å›¾åƒï¼Œç¡®ä¿è’¸é¦è®­ç»ƒä¸€è‡´æ€§
4. åŠ¨æ€åˆ†è¾¨çŽ‡: input_size å¯é…ç½® (é»˜è®¤ 640)
"""

import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List

import cv2
import numpy as np
import torch
import torch.nn.functional as F
from pycocotools import mask as mask_utils
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥ teacher
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å°è¯•å¯¼å…¥ Teacherï¼Œå¦‚æžœå¤±è´¥è¯·æ£€æŸ¥è·¯å¾„
try:
    from vfmkd.teachers.sam2_teacher import SAM2Teacher
    TEACHER_AVAILABLE = True
except ImportError:
    TEACHER_AVAILABLE = False
    print("Warning: Teacher modules not found. Ensure vfmkd/teachers is in python path.")


# =========================================================================
# 1. äºŒè¿›åˆ¶å†™å…¥ç®¡ç†å™¨ (BinWriter V2 - Hybrid)
# =========================================================================

class BinWriterV2:
    def __init__(self, output_dir, feat_dim, strides, save_edge=True, save_weight=True):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.feat_dim = feat_dim
        self.strides = sorted(strides)
        self.save_edge = save_edge
        self.save_weight = save_weight

        # æ‰“å¼€æ–‡ä»¶å¥æŸ„
        self.f_img = open(self.output_dir / "images.bin", "wb")
        self.f_feat = open(self.output_dir / "features.bin", "wb")
        self.f_bbox = open(self.output_dir / "bboxes.bin", "wb")
        self.f_mask = open(self.output_dir / "masks.bin", "wb")
        self.f_meta = open(self.output_dir / "metadata.bin", "wb")

        if self.save_edge:
            self.f_edge = open(self.output_dir / "edge_maps.bin", "wb")
        if self.save_weight:
            self.f_weight = open(self.output_dir / "weight_maps.bin", "wb")

        # ç´¢å¼•è¡¨ (offset, length)
        self.img_idx_list: List[List[int]] = []
        self.curr_img_offset = 0

        self.sample_count = 0
        self.keys_file = open(self.output_dir / "keys.txt", "w", encoding="utf-8")

    def write_batch(self, batch_data: List[Dict]):
        for item in batch_data:
            jpg_bytes = item["img_encoded"]
            length = len(jpg_bytes)

            self.img_idx_list.append([self.curr_img_offset, length])
            self.f_img.write(jpg_bytes)
            self.curr_img_offset += length

            for s in self.strides:
                feat = item["features"][s]
                self.f_feat.write(feat.tobytes())

            if self.save_edge:
                for s in self.strides:
                    edge = item["edge_maps"][s]
                    self.f_edge.write(edge.tobytes())

            if self.save_weight:
                for s in self.strides:
                    wm = item["weight_maps"][s]
                    self.f_weight.write(wm["fg"].tobytes())
                    self.f_weight.write(wm["bg"].tobytes())

            self.f_bbox.write(item["bbox"].tobytes())
            self.f_mask.write(item["mask"].tobytes())
            self.f_meta.write(item["meta"].tobytes())

            self.keys_file.write(f"{item['image_id']}\n")
            self.sample_count += 1

    def close(self):
        self.f_img.close()
        self.f_feat.close()
        self.f_bbox.close()
        self.f_mask.close()
        self.f_meta.close()
        if self.save_edge:
            self.f_edge.close()
        if self.save_weight:
            self.f_weight.close()
        self.keys_file.close()

        idx_arr = np.array(self.img_idx_list, dtype=np.int64)
        np.save(self.output_dir / "images.idx.npy", idx_arr)
        print(f"âœ… Saved Image Index: {len(idx_arr)} entries")

    def save_config(self, args, actual_input_size):
        config = {
            "protocol": "v2.0",
            "dataset_info": {
                "total_samples": self.sample_count,
                "description": f"Generated by ExtractV2 with {args.teacher_model}",
                "teacher_model": args.teacher_model,
                "teacher_device": args.device,
            },
            "resolution": {
                "input_size": actual_input_size,
                "resize_method": "cv2.INTER_LINEAR",
                "keep_aspect_ratio": False,
                "padding_value": 0,
            },
            "storage": {
                "feature_dim": self.feat_dim,
                "strides": self.strides,
                "save_edge": self.save_edge,
                "save_weight": self.save_weight,
                "image_format": "jpg_variable_index",
                "mask_size": 256,
            },
        }
        with open(self.output_dir / "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2)
        print("âœ… Saved Config JSON")


# =========================================================================
# 2. å¤„ç†å™¨ (Processor) - è´Ÿè´£ Resize å’Œ Mask ç”Ÿæˆ
# =========================================================================

class Processor:
    """CPU å¤„ç†å™¨ï¼šè´Ÿè´£å›¾åƒ Resizeã€ç¼–ç å’Œ Mask å¤„ç†"""

    def __init__(self, input_size=(640, 640), strides=None):
        self.input_h, self.input_w = input_size
        self.strides = strides or [4, 8, 16, 32]
        self.kernel = np.ones((3, 3), dtype=np.uint8)

    def process_item(self, img_path: Path, json_path: Path):
        try:
            image_id = img_path.stem
            img_bgr = cv2.imread(str(img_path))
            if img_bgr is None:
                return None

            orig_h, orig_w = img_bgr.shape[:2]
            img_resized = cv2.resize(
                img_bgr, (self.input_w, self.input_h), interpolation=cv2.INTER_LINEAR
            )

            success, img_encoded = cv2.imencode(
                ".jpg", img_resized, [cv2.IMWRITE_JPEG_QUALITY, 95]
            )
            if not success:
                return None

            with open(json_path, "r", encoding="utf-8") as f:
                ann_data = json.load(f)

            union_mask_orig = np.zeros((orig_h, orig_w), dtype=np.uint8)
            annotations = ann_data.get("annotations", [])

            for ann in annotations:
                rle = ann.get("segmentation")
                if rle:
                    mask = mask_utils.decode(rle)
                    union_mask_orig = np.maximum(union_mask_orig, mask)

            union_mask_input = cv2.resize(
                union_mask_orig, (self.input_w, self.input_h), interpolation=cv2.INTER_NEAREST
            )

            edge_maps = {}
            weight_maps = {}
            union_float = union_mask_input.astype(np.float32)

            for s in self.strides:
                h, w = self.input_h // s, self.input_w // s
                m_level = cv2.resize(union_float, (w, h), interpolation=cv2.INTER_NEAREST)

                edge = cv2.morphologyEx(m_level, cv2.MORPH_GRADIENT, self.kernel)
                edge_maps[s] = (edge > 0).astype(np.uint8)

                fg = m_level.astype(np.float32)
                bg = (1.0 - fg).astype(np.float32)
                weight_maps[s] = {"fg": fg, "bg": bg}

            mask_256 = cv2.resize(
                union_mask_orig, (256, 256), interpolation=cv2.INTER_NEAREST
            ).reshape(1, 256, 256)

            bbox = np.array([[0, 0, 1, 1]], dtype=np.float32)
            if annotations and "bbox" in annotations[0]:
                bx, by, bw, bh = annotations[0]["bbox"]
                scale_x = self.input_w / orig_w
                scale_y = self.input_h / orig_h
                x1 = bx * scale_x
                y1 = by * scale_y
                x2 = (bx + bw) * scale_x
                y2 = (by + bh) * scale_y
                bbox = np.array([[x1, y1, x2, y2]], dtype=np.float32)

            meta = np.array([len(annotations), 1, orig_h, orig_w, 3], dtype=np.int32)

            return {
                "image_id": image_id,
                "img_tensor_input": img_resized,
                "write_data": {
                    "image_id": image_id,
                    "img_encoded": img_encoded.tobytes(),
                    "edge_maps": edge_maps,
                    "weight_maps": weight_maps,
                    "bbox": bbox,
                    "mask": mask_256,
                    "meta": meta,
                },
            }
        except Exception:
            return None


# =========================================================================
# 3. è¾…åŠ©å‡½æ•°
# =========================================================================

def tensor_to_numpy(tensor: torch.Tensor) -> np.ndarray:
    return tensor.detach().cpu().numpy().astype(np.float32)


def interpolate_numpy(feat_np: np.ndarray, scale_factor: float) -> np.ndarray:
    tensor = torch.from_numpy(feat_np).unsqueeze(0)
    resized = F.interpolate(
        tensor, scale_factor=scale_factor, mode="bilinear", align_corners=False
    )
    return resized.squeeze(0).cpu().numpy().astype(np.float32)


def resample_feature(feat_np: np.ndarray, source_stride: int, target_stride: int) -> np.ndarray:
    scale = source_stride / target_stride
    return interpolate_numpy(feat_np, scale_factor=scale)


def build_item_features(
    feats_dict: Dict[str, torch.Tensor], idx: int, target_strides: List[int]
) -> Dict[int, np.ndarray]:
    item_feats: Dict[int, np.ndarray] = {}

    if "P4_S16" in feats_dict:
        item_feats[16] = tensor_to_numpy(feats_dict["P4_S16"][idx])
    elif "IMAGE_EMB_S16" in feats_dict:
        item_feats[16] = tensor_to_numpy(feats_dict["IMAGE_EMB_S16"][idx])

    if "P5_S32" in feats_dict:
        item_feats[32] = tensor_to_numpy(feats_dict["P5_S32"][idx])

    if "P3_S8" in feats_dict:
        item_feats[8] = tensor_to_numpy(feats_dict["P3_S8"][idx])

    for stride in target_strides:
        if stride in item_feats:
            continue
        if not item_feats:
            raise RuntimeError("Teacher è¿”å›žç‰¹å¾ä¸ºç©ºï¼Œæ— æ³•å†™å…¥ datasetã€‚")
        source_stride = min(item_feats.keys(), key=lambda s: abs(s - stride))
        item_feats[stride] = resample_feature(item_feats[source_stride], source_stride, stride)

    return item_feats


# =========================================================================
# 4. ä¸»æµç¨‹
# =========================================================================

def main():
    parser = argparse.ArgumentParser(description="SA-1B Extract V2 (Hybrid Storage)")
    parser.add_argument("--data-dir", required=True, help="Input data directory (jpg + json)")
    parser.add_argument("--output-dir", required=True, help="Output directory")
    parser.add_argument(
        "--input-size", type=int, default=640, help="Input resolution (square), e.g. 640 or 1024"
    )
    parser.add_argument("--teacher-model", default="sam2.1_hiera_b+", help="Teacher model type")
    parser.add_argument(
        "--checkpoint", default="weights/sam2.1_hiera_base_plus.pt", help="Path to checkpoint"
    )
    parser.add_argument("--device", default="cuda:0")
    parser.add_argument("--batch-size", type=int, default=8)
    parser.add_argument("--max-images", type=int, default=None)
    args = parser.parse_args()

    if not TEACHER_AVAILABLE:
        print("âŒ Error: Teacher module not found.")
        return

    print("\nðŸš€ SA-1B Extract V2 Starting...")
    print(f"   - Input Size: {args.input_size}x{args.input_size}")
    print(f"   - Teacher: {args.teacher_model}")
    print(f"   - Output: {args.output_dir}")

    print("ðŸ”¥ Loading Teacher...")
    teacher = SAM2Teacher(
        {
            "model_type": args.teacher_model,
            "checkpoint_path": args.checkpoint,
            "device": args.device,
        }
    )

    feat_dim = 256
    if hasattr(teacher, "get_feature_dims"):
        dims = teacher.get_feature_dims()
        if isinstance(dims, dict):
            feat_dim = int(dims.get("S16", dims.get("IMAGE_EMB", feat_dim)))

    strides = [4, 8, 16, 32]
    print(f"   - Detected Feature Dim: {feat_dim}")
    print(f"   - Target Strides: {strides}")

    writer = BinWriterV2(args.output_dir, feat_dim=feat_dim, strides=strides)
    processor = Processor(input_size=(args.input_size, args.input_size), strides=strides)

    data_dir = Path(args.data_dir)
    images = sorted(data_dir.glob("*.jpg"))
    if args.max_images:
        images = images[: args.max_images]
    print(f"ðŸ“Š Found {len(images)} images")

    batch_imgs: List[np.ndarray] = []
    batch_write_items: List[Dict] = []

    def flush_batch():
        nonlocal batch_imgs, batch_write_items
        if not batch_imgs:
            return
        batch_np = np.stack(batch_imgs)
        with torch.no_grad():
            feats_dict = teacher.extract_features(batch_np, save_features=False)

        for i in range(len(batch_imgs)):
            item = batch_write_items[i]
            item["features"] = build_item_features(feats_dict, i, strides)
        writer.write_batch(batch_write_items)
        batch_imgs = []
        batch_write_items = []

    for img_path in tqdm(images, desc="Processing"):
        json_path = img_path.with_suffix(".json")
        if not json_path.exists():
            continue

        processed = processor.process_item(img_path, json_path)
        if processed is None:
            continue

        img_rgb = cv2.cvtColor(processed["img_tensor_input"], cv2.COLOR_BGR2RGB)
        batch_imgs.append(img_rgb)
        batch_write_items.append(processed["write_data"])

        if len(batch_imgs) >= args.batch_size:
            flush_batch()

    flush_batch()

    writer.close()
    writer.save_config(args, [args.input_size, args.input_size])
    print(f"\nâœ… Extract V2 Finished. Output in {args.output_dir}")


if __name__ == "__main__":
    main()

