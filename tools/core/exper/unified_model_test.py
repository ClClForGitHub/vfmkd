#!/usr/bin/env python3
"""
ç‹¬ç«‹ç»Ÿä¸€æµ‹è¯•è„šæœ¬ï¼šåœ¨å›ºå®šæµ‹è¯•é›†ä¸Šï¼Œä½¿ç”¨ç»Ÿä¸€æŒ‡æ ‡ï¼ˆFeature MSE/MAEã€Cosineã€Edge BCE+Diceï¼‰
å¯¹å·²è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå…¬å¹³å¯¹æ¯”ã€‚

ä¸¥æ ¼å¤ç”¨å½“å‰è®­ç»ƒè„šæœ¬çš„å…³é”®é€»è¾‘ï¼š
- å¤ç”¨æ•°æ®é›†ç±» NPZWithImageIdDatasetï¼ˆçœŸå®å›¾ç‰‡+NPZä¸¥æ ¼é…å¯¹ï¼‰
- å¤ç”¨æ¨¡å‹ç»„ä»¶åˆ›å»ºæ–¹å¼ï¼ˆYOLOv8Backboneã€UniversalEdgeHeadã€SimpleAdapterï¼‰
- å¤ç”¨ç»Ÿä¸€æŒ‡æ ‡è¯„ä¼°å®ç° validate_unified_metrics çš„æ ¸å¿ƒè®¡ç®—æµç¨‹

ç”¨æ³•ç¤ºä¾‹ï¼š
python tools/core/unified_model_test.py \
  --features-dir /home/team/zouzhiyuan/dataset/sa1b/extracted \
  --images-dir /home/team/zouzhiyuan/dataset/sa1b \
  --checkpoints \
    outputs/distill_single_test_MSE/xxx/models/epoch_2_model.pth \
    outputs/distill_single_test_FGD/xxx/models/epoch_2_model.pth \
  --names MSE FGD \
  --batch-size 4 \
  --output outputs/unified_model_test.txt
"""

import os
import sys
import argparse
from pathlib import Path
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from torch.utils.data import DataLoader, Subset

# è·¯å¾„ï¼šå°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥sys.pathï¼Œä¿æŒä¸è®­ç»ƒè„šæœ¬ä¸€è‡´
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# ç›´æ¥å¯¼å…¥ï¼Œå› ä¸ºå·²ç»åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹
# ä½¿ç”¨importlibå¯¼å…¥ï¼Œé¿å…è·¯å¾„é—®é¢˜
import importlib.util
spec = importlib.util.spec_from_file_location(
    "train_distill_single_test",
    Path(__file__).parent / "train_distill_single_test.py"
)
train_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(train_module)
NPZWithImageIdDataset = train_module.NPZWithImageIdDataset
DistillSingleTester = train_module.DistillSingleTester


@torch.no_grad()
def evaluate_checkpoint(
    checkpoint_path: Path,
    test_features_dir: Path,
    test_images_dir: Path,
    batch_size: int = 4,
    max_images: int = None,
    gt_json_dir: Path | None = None,
) -> dict:
    """
    åŠ è½½checkpointï¼Œæ„å»ºä¸è®­ç»ƒä¸€è‡´çš„æ¨¡å‹ç»„ä»¶ï¼Œç„¶ååœ¨å›ºå®šæµ‹è¯•é›†ä¸Šç”¨ç»Ÿä¸€æŒ‡æ ‡è¯„ä¼°ã€‚
    ä½¿ç”¨å›ºå®šçš„1000å¼ æµ‹è¯•é›†ï¼ˆtestç›®å½•ï¼‰ï¼Œæˆ–ä»è®­ç»ƒé›†ä¸­é€‰æ‹©æŒ‡å®šæ•°é‡çš„æ ·æœ¬ã€‚
    è¿”å›ï¼š{"mse":..., "mae":..., "cosine_sim":..., "edge_loss":...}
    """
    # ä½¿ç”¨æµ‹è¯•é›†æˆ–è®­ç»ƒé›†ï¼ˆå¯é€šè¿‡max_imagesé™åˆ¶æ•°é‡ï¼‰
    test_dataset = NPZWithImageIdDataset(str(test_features_dir), str(test_images_dir), max_images=max_images, input_size=1024)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

    # æ„å»ºä¸è®­ç»ƒä¸€è‡´çš„æ¨¡å‹å®¹å™¨ï¼ˆä¸åšè®­ç»ƒï¼Œåªç”¨äºè¯„ä¼°ï¼‰
    config = {
        "backbone": "yolov8",
        # ä¸è®­ç»ƒä¸€è‡´çš„edgeæŸå¤±è®¾ç½®ï¼ˆç»Ÿä¸€æŒ‡æ ‡ä½¿ç”¨BCE+Diceï¼Œä¸å¯ç”¨æ©ç /pos_weightï¼‰
        "bce_weight": 0.5,
        "dice_weight": 0.5,
        "edge_mask_kernel_size": 3,
        "use_pos_weight": False,
        "enable_edge_mask_progressive": False,
    }
    runner = DistillSingleTester(config)

    # åŠ è½½checkpointï¼ˆä¸¥æ ¼å¯¹åº”é”®åï¼Œä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    ckpt = torch.load(str(checkpoint_path), map_location=runner.device)
    
    # åŠ è½½backbone
    runner.backbone.load_state_dict(ckpt["backbone"])
    
    # åŠ è½½edge_headï¼ˆç®€åŒ–ç‰ˆï¼Œå›ºå®š256é€šé“è¾“å…¥ï¼‰
    runner.edge_head.load_state_dict(ckpt["edge_head"], strict=False)
    
    # åŠ è½½é™æ€é€‚é…å™¨ï¼ˆç›´æ¥load_state_dictï¼Œå…¼å®¹æ—§V1é”®åï¼Œstrict=Falseï¼‰
    if "edge_adapter" in ckpt:
        runner.edge_adapter.load_state_dict(ckpt["edge_adapter"], strict=False)
    else:
        print("[WARNING] checkpointä¸­æ²¡æœ‰edge_adapterï¼ˆå¯èƒ½æ˜¯æ—§æ ¼å¼ï¼‰ï¼Œå·²è·³è¿‡")
    if "feature_adapter" in ckpt:
        runner.feature_adapter.load_state_dict(ckpt["feature_adapter"], strict=False)

    # ä½¿ç”¨è®­ç»ƒè„šæœ¬å†…çš„ç»Ÿä¸€æŒ‡æ ‡å®ç°ï¼Œä¿æŒè®¡ç®—ç»†èŠ‚ä¸€è‡´
    metrics = runner.validate_unified_metrics(test_loader)

    # å¯é€‰ï¼šè¡¥å……è®¡ç®—FGDä¸FSDçš„lossï¼ˆç”¨äºæ¨ªå‘å¯¹æ¯”ä¸åŒè’¸é¦æŒ‡æ ‡çš„å€¾å‘ï¼‰
    # ä¼˜å…ˆä»NPZè¯»å–é¢„è®¡ç®—æƒé‡ï¼ˆä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰ï¼Œç¼ºå¤±æ—¶ä¸”æä¾›äº†gt_json_diræ—¶å†å›é€€JSON
    metrics_fgd, metrics_fsd = None, None
    # å»¶è¿Ÿå¯¼å…¥æŸå¤±ä¸gt/npzå·¥å…·
    from vfmkd.distillation.losses.fgd_loss import FGDLoss
    from vfmkd.distillation.losses.fsd_loss import FSDLikeLoss
    from vfmkd.distillation.gt_adapter import build_fg_bg_from_ids
    # NPZä¼˜å…ˆå¯¼å…¥ï¼ˆå¯èƒ½ä¸å¯ç”¨æ—¶å†å›é€€JSONï¼‰
    try:
        from vfmkd.distillation.gt_adapter import load_weights_from_npz, load_edge_maps_from_npz
    except Exception:
        load_weights_from_npz = None
        load_edge_maps_from_npz = None

    fgd_loss_fn = FGDLoss(alpha_fg=0.001, beta_bg=0.0005, alpha_edge=0.002, gamma_mask=0.0, lambda_rela=0.0, temperature=1.0).to(runner.device)
    fsd_loss_fn = FSDLikeLoss(weight_fg=1.0, weight_bg=0.2, temperature=1.0, gamma_mask=0.0, lambda_rela=0.0, gaussian_from_mask=False, gaussian_mix="max", gaussian_blend_lambda=0.5).to(runner.device)

    total_fgd, total_fsd, total_n = 0.0, 0.0, 0
    for batch in test_loader:
        images = batch["image"].to(runner.device)
        teacher_features = batch["teacher_features"].to(runner.device)
        image_ids = batch["image_id"] if isinstance(batch["image_id"], list) else [*batch["image_id"]]

        # å‰å‘å¾—åˆ°å­¦ç”Ÿç‰¹å¾ï¼ˆS16â†’å¯¹é½åˆ°æ•™å¸ˆå°ºåº¦ï¼‰
        feats = runner.backbone(images)
        s16 = feats[2]
        aligned = runner.feature_adapter(s16)
        if aligned.shape[-2:] != teacher_features.shape[-2:]:
            aligned = F.interpolate(aligned, size=teacher_features.shape[-2:], mode="bilinear", align_corners=False)

        # æ„å»ºå‰æ™¯/èƒŒæ™¯æƒé‡å›¾åˆ°ç‰¹å¾åˆ†è¾¨ç‡ï¼šä¼˜å…ˆNPZï¼ˆtest_features_dirï¼‰ï¼Œç¼ºå¤±æ—¶JSONï¼ˆè‹¥æä¾›ï¼‰
        Hf, Wf = aligned.shape[-2], aligned.shape[-1]
        fg_map, bg_map = None, None
        if load_weights_from_npz is not None:
            try:
                fg_map, bg_map = load_weights_from_npz(image_ids, str(test_features_dir), (Hf, Wf))
                fg_map = fg_map.to(runner.device)
                bg_map = bg_map.to(runner.device)
            except Exception:
                fg_map, bg_map = None, None
        if (fg_map is None or bg_map is None) and gt_json_dir is not None:
            fg_map, bg_map = build_fg_bg_from_ids(image_ids, str(gt_json_dir), (Hf, Wf))
            fg_map = fg_map.to(runner.device)
            bg_map = bg_map.to(runner.device)

        # è‹¥ä»ä¸å¯ç”¨ï¼ˆæ—¢æ²¡æœ‰NPZä¹Ÿæ²¡æœ‰JSONï¼‰ï¼Œåˆ™æ— æ³•è®¡ç®—FGD/FSDï¼Œè·³è¿‡è¯¥batch
        if fg_map is None or bg_map is None:
            continue

        # åˆ†åˆ«è®¡ç®—FGD/FSDçš„lossï¼ˆå‡å€¼åŒ–ï¼‰
        fgd_val = fgd_loss_fn(aligned, teacher_features, fg_map=fg_map, bg_map=bg_map, edge_map=None).item()
        fsd_val = fsd_loss_fn(aligned, teacher_features, fg_map=fg_map, bg_map=bg_map).item()

        bs = images.size(0)
        total_fgd += fgd_val * bs
        total_fsd += fsd_val * bs
        total_n += bs

    if total_n > 0:
        metrics_fgd = total_fgd / total_n
        metrics_fsd = total_fsd / total_n

    # åˆå¹¶æ‰©å±•æŒ‡æ ‡
    if metrics_fgd is not None:
        metrics["fgd_loss"] = metrics_fgd
    if metrics_fsd is not None:
        metrics["fsd_loss"] = metrics_fsd

    # è¿”å›runnerå¯¹è±¡ç”¨äºå¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    return metrics, runner


@torch.no_grad()
def load_runner_for_visualization(
    checkpoint_path: Path,
    test_features_dir: Path,
    test_images_dir: Path,
    batch_size: int = 4,
    max_images: int = None,
):
    """ä»…ç”¨äºå¯è§†åŒ–ï¼šåŠ è½½runnerä¸æ•°æ®é›†ï¼Œä¸è¿›è¡Œä»»ä½•è¯„ä¼°æˆ–æŒ‡æ ‡è®¡ç®—ã€‚"""
    # ä»…æ„å»ºæ•°æ®é›†ï¼ˆé™åˆ¶æ ·æœ¬æ•°ç”¨äºå¯è§†åŒ–ï¼‰
    test_dataset = NPZWithImageIdDataset(str(test_features_dir), str(test_images_dir), max_images=max_images, input_size=1024)

    # æ„å»ºrunnerï¼ˆä¸è®­ç»ƒä¸€è‡´çš„ç»„ä»¶ï¼‰
    config = {
        "backbone": "yolov8",
        "bce_weight": 0.5,
        "dice_weight": 0.5,
        "edge_mask_kernel_size": 3,
        "use_pos_weight": False,
        "enable_edge_mask_progressive": False,
    }
    runner = DistillSingleTester(config)

    # åŠ è½½checkpointæƒé‡
    ckpt = torch.load(str(checkpoint_path), map_location=runner.device)
    runner.backbone.load_state_dict(ckpt["backbone"])  # backbone
    runner.edge_head.load_state_dict(ckpt.get("edge_head", {}), strict=False)  # edge head

    # åŠ è½½é™æ€é€‚é…å™¨ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    if "edge_adapter" in ckpt:
        runner.edge_adapter.load_state_dict(ckpt["edge_adapter"], strict=False)
    if "feature_adapter" in ckpt:
        runner.feature_adapter.load_state_dict(ckpt["feature_adapter"], strict=False)

    return runner, test_dataset


def visualize_comparison(results, test_dataset, output_dir: Path, num_samples: int):
    """
    å¹¶æ’å¯¹æ¯”æ¨¡å¼ï¼šæ¯ä¸ªæ ·æœ¬ä¸€å¼ å¤§å›¾ï¼Œå¤šæ¨¡å‹å¹¶æ’å¯¹æ¯”
    å¸ƒå±€ï¼š(num_models + 1)è¡Œ x 5åˆ—
    è¡Œï¼šGT + å„æ¨¡å‹é¢„æµ‹
    åˆ—ï¼šåŸå›¾ã€è¾¹ç¼˜GT/é¢„æµ‹ã€è¾¹ç¼˜å åŠ ã€è¾¹ç¼˜è¯¯å·®ã€ç‰¹å¾å¯¹æ¯”
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    num_models = len(results)
    print(f"ğŸ–¼ï¸  Generating {num_samples} comparison visualizations ({num_models} models)...")
    
    with torch.no_grad():
        for idx in range(min(num_samples, len(test_dataset))):
            sample = test_dataset[idx]
            image = sample['image'].unsqueeze(0)
            teacher_features = sample['teacher_features'].unsqueeze(0)
            edge_gt = sample['edge_256x256']
            image_id = sample['image_id']
            
            # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœ
            model_predictions = {}
            for result in results:
                runner = result['runner']
                runner.backbone.eval()
                runner.edge_adapter.eval()
                runner.edge_head.eval()
                runner.feature_adapter.eval()
                
                # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                image_t = image.to(runner.device)
                teacher_features_t = teacher_features.to(runner.device)
                
                # å‰å‘ä¼ æ’­
                features = runner.backbone(image_t)
                s4_features = features[0]
                s16_features = features[2]
                
                aligned_s4 = runner.edge_adapter(s4_features)
                edge_logits = runner.edge_head(aligned_s4)
                edge_pred = torch.sigmoid(edge_logits[0, 0]).cpu().numpy()
                
                aligned_features = runner.feature_adapter(s16_features)
                if aligned_features.shape[-2:] != teacher_features_t.shape[-2:]:
                    aligned_features = F.interpolate(aligned_features, size=teacher_features_t.shape[-2:], mode="bilinear", align_corners=False)
                p4_feat = aligned_features[0].cpu().numpy()
                p4_mean = p4_feat.mean(axis=0)
                
                model_predictions[result['name']] = {
                    'edge_pred': edge_pred,
                    'p4_mean': p4_mean,
                }
            
            # å‡†å¤‡å›¾åƒ
            img_np = image[0].cpu().numpy().transpose(1, 2, 0)
            img_np = np.clip(img_np, 0, 1)
            img_resized = F.interpolate(
                torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0),
                size=(256, 256), mode='bilinear', align_corners=False
            )[0].permute(1, 2, 0).numpy()
            
            edge_gt_np = edge_gt.numpy()
            teacher_feat = teacher_features[0].cpu().numpy()
            teacher_mean = teacher_feat.mean(axis=0)
            
            # åˆ›å»ºå¯¹æ¯”å›¾ï¼š(num_models+1)è¡Œ x 5åˆ—ï¼ˆ+1æ˜¯å› ä¸ºGTè¡Œï¼‰
            fig = plt.figure(figsize=(25, 5 * (num_models + 1)))
            gs = GridSpec(num_models + 1, 5, figure=fig, hspace=0.3, wspace=0.3)
            
            # ç¬¬ä¸€è¡Œï¼šGTä½œä¸ºå‚è€ƒ
            row = 0
            ax = fig.add_subplot(gs[row, 0])
            ax.imshow(img_np)
            ax.set_title(f"Input Image\n(ID: {image_id})", fontsize=12, fontweight='bold')
            ax.axis('off')
            
            ax = fig.add_subplot(gs[row, 1])
            ax.imshow(edge_gt_np, cmap='gray', vmin=0, vmax=1)
            ax.set_title("Edge GT", fontsize=12, fontweight='bold')
            ax.axis('off')
            
            ax = fig.add_subplot(gs[row, 2])
            ax.imshow(img_resized)
            ax.contour(edge_gt_np, levels=[0.5], colors='green', linewidths=2, alpha=0.8)
            ax.set_title("GT Overlay", fontsize=12, fontweight='bold')
            ax.axis('off')
            
            ax = fig.add_subplot(gs[row, 3])
            ax.text(0.5, 0.5, "Ground Truth\nReference", ha='center', va='center', 
                   fontsize=16, fontweight='bold')
            ax.axis('off')
            
            ax = fig.add_subplot(gs[row, 4])
            im = ax.imshow(teacher_mean, cmap='viridis')
            ax.set_title(f"Teacher Feature\nmean={teacher_mean.mean():.3f}", fontsize=12)
            plt.colorbar(im, ax=ax, fraction=0.046)
            ax.axis('off')
            
            # åç»­è¡Œï¼šå„æ¨¡å‹çš„é¢„æµ‹
            for row, result in enumerate(results, start=1):
                model_name = result['name']
                pred = model_predictions[model_name]
                edge_pred = pred['edge_pred']
                p4_mean = pred['p4_mean']
                
                # åŸå›¾ï¼ˆåªåœ¨ç¬¬ä¸€è¡Œæ˜¾ç¤ºï¼Œå…¶ä»–è¡Œç©ºç™½æˆ–æ˜¾ç¤ºæ¨¡å‹åï¼‰
                ax = fig.add_subplot(gs[row, 0])
                if row == 1:
                    ax.imshow(img_np)
                ax.text(0.5, 0.5, model_name, ha='center', va='center', 
                       fontsize=14, fontweight='bold')
                ax.axis('off')
                
                # è¾¹ç¼˜é¢„æµ‹
                ax = fig.add_subplot(gs[row, 1])
                ax.imshow(edge_pred, cmap='gray', vmin=0, vmax=1)
                ax.set_title(f"Edge Prediction\nmean={edge_pred.mean():.3f}", fontsize=11)
                ax.axis('off')
                
                # è¾¹ç¼˜å åŠ 
                ax = fig.add_subplot(gs[row, 2])
                ax.imshow(img_resized)
                ax.contour(edge_pred, levels=[0.5], colors='red', linewidths=2, alpha=0.8)
                ax.set_title("Pred Overlay", fontsize=11)
                ax.axis('off')
                
                # è¾¹ç¼˜è¯¯å·®
                ax = fig.add_subplot(gs[row, 3])
                edge_diff = np.abs(edge_pred - edge_gt_np)
                im = ax.imshow(edge_diff, cmap='hot', vmin=0, vmax=1)
                ax.set_title(f"Edge Error\nMAE={edge_diff.mean():.3f}", fontsize=11)
                plt.colorbar(im, ax=ax, fraction=0.046)
                ax.axis('off')
                
                # ç‰¹å¾å¯¹æ¯”
                ax = fig.add_subplot(gs[row, 4])
                feat_diff = np.abs(p4_mean - teacher_mean)
                im = ax.imshow(feat_diff, cmap='hot')
                ax.set_title(f"Feature Diff\nMAE={feat_diff.mean():.3f}", fontsize=11)
                plt.colorbar(im, ax=ax, fraction=0.046)
                ax.axis('off')
            
            # ä¿å­˜
            save_path = output_dir / f"{image_id}_comparison.png"
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()
            print(f"  âœ… {idx+1}/{num_samples}: {save_path.name}")
    
    print(f"\nğŸ‰ Comparison visualizations complete!")


def visualize_separate(results, test_dataset, output_dir: Path, num_samples: int):
    """
    åˆ†åˆ«ä¿å­˜æ¨¡å¼ï¼šæ¯ä¸ªæ¨¡å‹æ¯ä¸ªæ ·æœ¬åˆ†åˆ«ä¿å­˜ï¼ˆå¤ç”¨åŸæœ‰å¯è§†åŒ–é€»è¾‘ï¼‰
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ–¼ï¸  Generating {num_samples} separate visualizations for {len(results)} models...")
    
    with torch.no_grad():
        for result in results:
            model_name = result['name']
            runner = result['runner']
            runner.backbone.eval()
            runner.edge_adapter.eval()
            runner.edge_head.eval()
            runner.feature_adapter.eval()
            
            model_output_dir = output_dir / model_name
            model_output_dir.mkdir(parents=True, exist_ok=True)
            
            for idx in range(min(num_samples, len(test_dataset))):
                sample = test_dataset[idx]
                image = sample['image'].unsqueeze(0).to(runner.device)
                teacher_features = sample['teacher_features'].unsqueeze(0).to(runner.device)
                edge_gt = sample['edge_256x256']
                image_id = sample['image_id']
                
                # å‰å‘ä¼ æ’­
                features = runner.backbone(image)
                s4_features = features[0]
                s16_features = features[2]
                
                aligned_s4 = runner.edge_adapter(s4_features)
                edge_logits = runner.edge_head(aligned_s4)
                edge_pred = torch.sigmoid(edge_logits[0, 0]).cpu().numpy()
                
                aligned_features = runner.feature_adapter(s16_features)
                if aligned_features.shape[-2:] != teacher_features.shape[-2:]:
                    aligned_features = F.interpolate(aligned_features, size=teacher_features.shape[-2:], mode="bilinear", align_corners=False)
                p4_feat = aligned_features[0].cpu().numpy()
                p4_mean = p4_feat.mean(axis=0)
                p4_energy = np.sqrt((p4_feat ** 2).mean(axis=0))
                
                teacher_feat = teacher_features[0].cpu().numpy()
                teacher_mean = teacher_feat.mean(axis=0)
                
                img_np = image[0].cpu().numpy().transpose(1, 2, 0)
                img_np = np.clip(img_np, 0, 1)
                
                # åˆ›å»ºå¯è§†åŒ–ï¼ˆ2è¡Œ5åˆ—ï¼Œä¸åŸæœ‰é€»è¾‘ä¸€è‡´ï¼‰
                fig = plt.figure(figsize=(20, 10))
                gs = GridSpec(2, 5, figure=fig, hspace=0.3, wspace=0.3)
                
                # Row 1: Original, Edge GT, Edge Pred, Edge Overlay, Edge Error
                ax0 = fig.add_subplot(gs[0, 0])
                ax0.imshow(img_np)
                ax0.set_title(f"Input Image\n(ID: {image_id})", fontsize=10)
                ax0.axis('off')
                
                ax1 = fig.add_subplot(gs[0, 1])
                ax1.imshow(edge_gt.numpy(), cmap='gray', vmin=0, vmax=1)
                ax1.set_title("Edge GT (256x256)", fontsize=10)
                ax1.axis('off')
                
                ax2 = fig.add_subplot(gs[0, 2])
                ax2.imshow(edge_pred, cmap='gray', vmin=0, vmax=1)
                ax2.set_title(f"Edge Prediction\nmean={edge_pred.mean():.3f}", fontsize=10)
                ax2.axis('off')
                
                ax3 = fig.add_subplot(gs[0, 3])
                img_resized = F.interpolate(
                    torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0),
                    size=(256, 256), mode='bilinear', align_corners=False
                )[0].permute(1, 2, 0).numpy()
                ax3.imshow(img_resized)
                ax3.contour(edge_pred, levels=[0.5], colors='red', linewidths=2, alpha=0.8)
                ax3.set_title("Edge Overlay (th=0.5)", fontsize=10)
                ax3.axis('off')
                
                ax4 = fig.add_subplot(gs[0, 4])
                edge_diff = np.abs(edge_pred - edge_gt.numpy())
                im4 = ax4.imshow(edge_diff, cmap='hot', vmin=0, vmax=1)
                ax4.set_title(f"Edge Error\nMAE={edge_diff.mean():.3f}", fontsize=10)
                plt.colorbar(im4, ax=ax4, fraction=0.046)
                ax4.axis('off')
                
                # Row 2: P4 Mean, P4 Energy, Teacher Mean, Feature Diff, Channel Grid
                ax5 = fig.add_subplot(gs[1, 0])
                im5 = ax5.imshow(p4_mean, cmap='viridis')
                ax5.set_title(f"Student P4 Mean\nmean={p4_mean.mean():.3f}", fontsize=10)
                plt.colorbar(im5, ax=ax5, fraction=0.046)
                ax5.axis('off')
                
                ax6 = fig.add_subplot(gs[1, 1])
                im6 = ax6.imshow(p4_energy, cmap='hot')
                ax6.set_title(f"P4 Energy Map\nmax={p4_energy.max():.3f}", fontsize=10)
                plt.colorbar(im6, ax=ax6, fraction=0.046)
                ax6.axis('off')
                
                ax7 = fig.add_subplot(gs[1, 2])
                im7 = ax7.imshow(teacher_mean, cmap='viridis')
                ax7.set_title(f"Teacher SAM Mean\nmean={teacher_mean.mean():.3f}", fontsize=10)
                plt.colorbar(im7, ax=ax7, fraction=0.046)
                ax7.axis('off')
                
                ax8 = fig.add_subplot(gs[1, 3])
                feat_diff = np.abs(p4_mean - teacher_mean)
                im8 = ax8.imshow(feat_diff, cmap='hot')
                ax8.set_title(f"Feature Difference\nMAE={feat_diff.mean():.3f}", fontsize=10)
                plt.colorbar(im8, ax=ax8, fraction=0.046)
                ax8.axis('off')
                
                ax9 = fig.add_subplot(gs[1, 4])
                n_show = min(16, p4_feat.shape[0])
                grid_size = 4
                channel_grid = np.zeros((grid_size * 16, grid_size * 16))
                for i in range(n_show):
                    row, col = i // grid_size, i % grid_size
                    ch_data = p4_feat[i]
                    ch_resized = F.interpolate(
                        torch.from_numpy(ch_data).unsqueeze(0).unsqueeze(0),
                        size=(16, 16), mode='bilinear', align_corners=False
                    )[0, 0].numpy()
                    channel_grid[row*16:(row+1)*16, col*16:(col+1)*16] = ch_resized
                im9 = ax9.imshow(channel_grid, cmap='gray')
                ax9.set_title(f"First {n_show} Channels", fontsize=10)
                ax9.axis('off')
                
                # ä¿å­˜
                save_path = model_output_dir / f"{image_id}_{model_name}_visualization.png"
                plt.savefig(save_path, dpi=150, bbox_inches='tight')
                plt.close()
            
            print(f"  âœ… {model_name}: {num_samples} visualizations saved to {model_output_dir}")
    
    print(f"\nğŸ‰ Separate visualizations complete!")


def visualize_panel_4x4(results, test_dataset, output_dir: Path, num_samples: int):
    """
    4x4 æ±‡æ€»é¢æ¿ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸€å¼ å›¾ï¼‰ï¼š
    ç¬¬1è¡Œï¼šå››å¼ ç‰¹å¾å‡å€¼ï¼ˆå››ä¸ªæ¨¡å‹ï¼‰
    ç¬¬2è¡Œï¼šå››å¼ ç‰¹å¾å·®å¼‚ |student mean - teacher mean|
    ç¬¬3è¡Œï¼šå››å¼ è¾¹ç¼˜é¢„æµ‹
    ç¬¬4è¡Œï¼šå››å¼ è¾¹ç¼˜è¯¯å·® |pred - edge_gt|
    ä»…å±•ç¤ºè¿™å››ç±»å›¾ï¼Œå…¶å®ƒä¸éœ€è¦ã€‚
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # å›ºå®šåˆ—é¡ºåº
    desired_order = ["MSE_Baseline", "FGD_NoEdgeBoost", "FGD_EdgeBoost", "FSD_NoEdgeBoost"]
    name_to_result = {r['name']: r for r in results}
    model_results = [name_to_result[n] for n in desired_order if n in name_to_result]
    if len(model_results) < 4:
        # å›é€€ï¼šè¡¥é½ç¼ºå¤±çš„æŒ‰åŸé¡ºåºï¼ˆä»…å½“æœ‰ç¼ºå¤±æ—¶ï¼‰
        seen = set(r['name'] for r in model_results)
        for r in results:
            if r['name'] not in seen and len(model_results) < 4:
                model_results.append(r)

    with torch.no_grad():
        for idx in range(min(num_samples, len(test_dataset))):
            sample = test_dataset[idx]
            image = sample['image'].unsqueeze(0)
            teacher_features = sample['teacher_features'].unsqueeze(0)
            edge_gt = sample['edge_256x256'].numpy()
            image_id = sample['image_id']

            # è®¡ç®—teacher meanä¸€æ¬¡ï¼Œå¹¶è®¾å®šç»Ÿä¸€è‰²æ ‡èŒƒå›´ï¼ˆç”¨æ•™å¸ˆåˆ†å¸ƒåšå‚è€ƒï¼‰
            teacher_feat_np = teacher_features[0].cpu().numpy()
            teacher_mean = teacher_feat_np.mean(axis=0)
            vmin_mean = float(np.percentile(teacher_mean, 1))
            vmax_mean = float(np.percentile(teacher_mean, 99))

            # æ”¶é›†å››ä¸ªæ¨¡å‹çš„ï¼šp4_mean, feat_signed_diff, edge_pred, edge_err
            collected = []
            for result in model_results:
                runner = result['runner']
                runner.backbone.eval(); runner.edge_adapter.eval(); runner.edge_head.eval(); runner.feature_adapter.eval()

                img_t = image.to(runner.device)
                tea_t = teacher_features.to(runner.device)
                feats = runner.backbone(img_t)
                s4 = feats[0]; s16 = feats[2]
                aligned_s4 = runner.edge_adapter(s4)
                edge_logits = runner.edge_head(aligned_s4)
                edge_pred = torch.sigmoid(edge_logits[0, 0]).detach().cpu().numpy()

                aligned = runner.feature_adapter(s16)
                if aligned.shape[-2:] != tea_t.shape[-2:]:
                    aligned = F.interpolate(aligned, size=tea_t.shape[-2:], mode="bilinear", align_corners=False)
                p4_feat = aligned[0].detach().cpu().numpy()
                p4_mean = p4_feat.mean(axis=0)
                # ç­¾åå·®å¼‚ï¼ˆé›¶ä¸­å¿ƒæ˜¾ç¤ºæ›´ç›´è§‚ï¼‰ï¼šstudent_mean - teacher_mean
                feat_signed_diff = p4_mean - teacher_mean
                edge_err = np.abs(edge_pred - edge_gt)

                collected.append({
                    'name': result['name'],
                    'p4_mean': p4_mean,
                    'feat_signed_diff': feat_signed_diff,
                    'edge_pred': edge_pred,
                    'edge_err': edge_err,
                })

            # ç»˜åˆ¶ 5x4 é¢æ¿ï¼ˆé¡¶éƒ¨å‚è€ƒè¡Œ + å››è¡Œå¯¹æ¯”ï¼‰
            fig = plt.figure(figsize=(16, 20))
            gs = GridSpec(5, 4, figure=fig, hspace=0.15, wspace=0.15)

            # é¡¶éƒ¨å‚è€ƒè¡Œï¼šåŸå›¾ï¼ˆè·¨ä¸¤åˆ—ï¼‰ã€Teacher NPZ Meanã€NPZ Edge
            img_np = image[0].cpu().numpy().transpose(1, 2, 0)
            img_np = np.clip(img_np, 0, 1)
            ax = fig.add_subplot(gs[0, 0:2])
            ax.imshow(img_np)
            ax.set_title(f"Input Image (ID: {image_id})", fontsize=11, fontweight='bold')
            ax.axis('off')

            ax = fig.add_subplot(gs[0, 2])
            im = ax.imshow(teacher_mean, cmap='viridis', vmin=vmin_mean, vmax=vmax_mean)
            ax.set_title("Teacher NPZ Feature Mean", fontsize=10)
            ax.axis('off')
            plt.colorbar(im, ax=ax, fraction=0.035)

            ax = fig.add_subplot(gs[0, 3])
            ax.imshow(edge_gt, cmap='gray', vmin=0, vmax=1)
            ax.set_title("NPZ Edge (256x256)", fontsize=10)
            ax.axis('off')

            # é¡¶éƒ¨åˆ—åæ ‡ç­¾
            for c, item in enumerate(collected[:4]):
                ax = fig.add_subplot(gs[1, c])
                ax.set_title(item['name'], fontsize=11, fontweight='bold', pad=6)
                ax.axis('off')
            # è¡Œ1ï¼šç‰¹å¾å‡å€¼ï¼ˆå®é™…å†…å®¹æ”¾åœ¨å•ç‹¬ä¸€å±‚ï¼Œä½¿æ ‡é¢˜ä¸è¢«è¦†ç›–ï¼‰
            for c, item in enumerate(collected[:4]):
                ax = fig.add_subplot(gs[1, c])
                im = ax.imshow(item['p4_mean'], cmap='viridis', vmin=vmin_mean, vmax=vmax_mean)
                ax.set_title("P4 Mean", fontsize=10)
                ax.axis('off')
                plt.colorbar(im, ax=ax, fraction=0.035)

            # è¡Œ2ï¼šç‰¹å¾å·®å¼‚ï¼ˆç­¾åå·®å¼‚ï¼Œé›¶ä¸­å¿ƒåŒæè‰²å›¾ï¼‰
            diffs = np.stack([it['feat_signed_diff'] for it in collected[:4]], axis=0)
            diff_abs_max = float(np.max(np.abs(diffs))) + 1e-6
            for c, item in enumerate(collected[:4]):
                ax = fig.add_subplot(gs[2, c])
                im = ax.imshow(item['feat_signed_diff'], cmap='seismic', vmin=-diff_abs_max, vmax=diff_abs_max)
                ax.set_title("Mean âˆ’ Teacher (signed)", fontsize=10)
                ax.axis('off')
                plt.colorbar(im, ax=ax, fraction=0.035)

            # è¡Œ3ï¼šè¾¹ç¼˜é¢„æµ‹
            for c, item in enumerate(collected[:4]):
                ax = fig.add_subplot(gs[3, c])
                ax.imshow(item['edge_pred'], cmap='gray', vmin=0, vmax=1)
                ax.set_title(f"Edge Pred", fontsize=10)
                ax.axis('off')

            # è¡Œ4ï¼šè¾¹ç¼˜è¯¯å·®
            for c, item in enumerate(collected[:4]):
                ax = fig.add_subplot(gs[4, c])
                im = ax.imshow(item['edge_err'], cmap='hot', vmin=0, vmax=1)
                ax.set_title(f"Edge Error", fontsize=10)
                ax.axis('off')
                plt.colorbar(im, ax=ax, fraction=0.035)

            # æ€»æ ‡é¢˜+å›¾æ³¨
            fig.suptitle(
                "Top Ref: Input | Teacher NPZ Mean | NPZ Edge | "
                "Row1: P4 Mean | Row2: Meanâˆ’Teacher (signed, zero-centered) | "
                "Row3: Edge Pred | Row4: Edge Error\n"
                "Columns: MSE_Baseline | FGD_NoEdgeBoost | FGD_EdgeBoost | FSD_NoEdgeBoost",
                fontsize=12, y=0.96
            )

            save_path = output_dir / f"{image_id}_panel4x4.png"
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()
            print(f"  âœ… {idx+1}/{num_samples}: {save_path.name}")

    print("\nğŸ‰ Panel 4x4 visualizations complete!")

def main():
    parser = argparse.ArgumentParser(description="Unified Model Test - Fair evaluation with unified metrics")
    parser.add_argument("--test-features-dir", type=str, default="/home/team/zouzhiyuan/dataset/sa1b/test/extracted",
                       help="æµ‹è¯•é›†NPZç‰¹å¾ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨å›ºå®š1000å¼ æµ‹è¯•é›†ï¼‰")
    parser.add_argument("--test-images-dir", type=str, default="/home/team/zouzhiyuan/dataset/sa1b/test",
                       help="æµ‹è¯•é›†å›¾ç‰‡ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨å›ºå®š1000å¼ æµ‹è¯•é›†ï¼‰")
    parser.add_argument("--max-images", type=int, default=None,
                       help="é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆä¾‹å¦‚50ï¼Œä»æ•°æ®é›†ä¸­é€‰æ‹©å‰Nå¼ ï¼‰")
    parser.add_argument("--checkpoints", nargs="+", required=True, help="å¤šä¸ªæ¨¡å‹checkpointè·¯å¾„")
    parser.add_argument("--names", nargs="+", required=True, help="ä¸checkpointä¸€ä¸€å¯¹åº”çš„åç§°")
    parser.add_argument("--batch-size", type=int, default=4)
    parser.add_argument("--gt-json-dir", type=str, default=None, help="å¯é€‰ï¼šæä¾›SAé£æ ¼GT jsonç›®å½•ä»¥è®¡ç®—FGD/FSDæŒ‡æ ‡")
    parser.add_argument("--output", type=str, default="outputs/unified_model_test.txt")
    
    # å¯è§†åŒ–é€‰é¡¹
    parser.add_argument("--visualize", action="store_true", help="å¯ç”¨å¯è§†åŒ–ï¼Œç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”å›¾")
    parser.add_argument("--num-vis-samples", type=int, default=10, help="å¯è§†åŒ–æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤10ï¼‰")
    parser.add_argument("--vis-output-dir", type=str, default="outputs/visualizations",
                       help="å¯è§†åŒ–è¾“å‡ºç›®å½•")
    parser.add_argument("--vis-mode", type=str, default="comparison",
                       choices=["comparison", "separate", "panel"],
                       help="comparison: å¹¶æ’å¯¹æ¯”, separate: åˆ†åˆ«ä¿å­˜, panel: 4x4æ±‡æ€»é¢æ¿")
    parser.add_argument("--visualize-only", action="store_true",
                       help="ä»…è¿›è¡Œå¯è§†åŒ–ï¼ˆåŠ è½½æ¨¡å‹å¹¶å¯¹æŒ‡å®šæ•°é‡æ ·æœ¬ç”ŸæˆP4ç‰¹å¾ä¸è¾¹ç¼˜å›¾ï¼‰ï¼Œä¸åšä»»ä½•æŒ‡æ ‡è¯„ä¼°")
    
    args = parser.parse_args()

    if len(args.checkpoints) != len(args.names):
        raise ValueError("--checkpoints ä¸ --names æ•°é‡å¿…é¡»ä¸€è‡´")

    test_features_dir = Path(args.test_features_dir)
    test_images_dir = Path(args.test_images_dir)
    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    print("=" * 80)
    print("Unified Model Test - Start")
    print("=" * 80)
    print(f"test_features_dir: {test_features_dir}")
    print(f"test_images_dir  : {test_images_dir}")
    if args.max_images:
        print(f"max_images      : {args.max_images} (é™åˆ¶æ ·æœ¬æ•°é‡)")
    print(f"models           : {len(args.checkpoints)}")
    if args.visualize:
        print(f"visualize        : âœ… Enabled ({args.num_vis_samples} samples, mode={args.vis_mode})")
    print()

    results = []
    if args.visualize_only:
        # ä»…ä¸ºå¯è§†åŒ–åˆ›å»ºrunnerï¼Œä¸åšæŒ‡æ ‡è¯„ä¼°
        print("Visualization-only mode: skip metrics, only generate P4 feature and edge visualizations.")
        # ä¸ºæ¯ä¸ªcheckpointæ„å»ºrunnerå¹¶åŠ è½½æƒé‡ï¼ˆä¸åšä»»ä½•è¯„ä¼°ï¼‰
        results = []
        for name, ckpt in zip(args.names, args.checkpoints):
            ckpt_path = Path(ckpt)
            if not ckpt_path.exists():
                print(f"[WARN] checkpoint not found, skip: {ckpt_path}")
                continue
            print(f"Preparing model for visualization: {name}\n  ckpt: {ckpt_path}")
            runner_obj, test_dataset = load_runner_for_visualization(
                checkpoint_path=ckpt_path,
                test_features_dir=test_features_dir,
                test_images_dir=test_images_dir,
                batch_size=args.batch_size,
                max_images=args.max_images,
            )
            results.append({"name": name, "runner": runner_obj})
        # ç›´æ¥è¿›å…¥å¯è§†åŒ–
        if args.visualize:
            print("\n" + "=" * 80)
            print("Generating Visualizations (visualize-only mode)...")
            print("=" * 80)
            vis_output_dir = Path(args.vis_output_dir)
            vis_output_dir.mkdir(parents=True, exist_ok=True)
            num_vis = min(args.num_vis_samples, len(test_dataset))
            if args.vis_mode == "comparison":
                visualize_comparison(results, test_dataset, vis_output_dir, num_vis)
            elif args.vis_mode == "separate":
                visualize_separate(results, test_dataset, vis_output_dir, num_vis)
            else:
                visualize_panel_4x4(results, test_dataset, vis_output_dir, num_vis)
            print(f"\nâœ… Visualizations saved to: {vis_output_dir}")
        return
    else:
        for name, ckpt in zip(args.names, args.checkpoints):
            ckpt_path = Path(ckpt)
            if not ckpt_path.exists():
                print(f"[WARN] checkpoint not found, skip: {ckpt_path}")
                continue
            print(f"Evaluating: {name}\n  ckpt: {ckpt_path}")
            try:
                metrics_dict, runner_obj = evaluate_checkpoint(
                    checkpoint_path=ckpt_path,
                    test_features_dir=test_features_dir,
                    test_images_dir=test_images_dir,
                    batch_size=args.batch_size,
                    max_images=args.max_images,
                    gt_json_dir=Path(args.gt_json_dir) if args.gt_json_dir else None,
                )
                results.append({"name": name, **metrics_dict, "runner": runner_obj})
                print(f"  Feature MSE : {metrics_dict['mse']:.6f}")
                print(f"  Feature MAE : {metrics_dict['mae']:.6f}")
                print(f"  Cosine Sim  : {metrics_dict['cosine_sim']:.6f}")
                print(f"  Edge Loss   : {metrics_dict['edge_loss']:.6f}\n")
                if 'fgd_loss' in metrics_dict:
                    print(f"  FGD Loss    : {metrics_dict['fgd_loss']:.6f}")
                if 'fsd_loss' in metrics_dict:
                    print(f"  FSD Loss    : {metrics_dict['fsd_loss']:.6f}")
            except Exception as e:
                print(f"  [ERROR] evaluate failed: {e}\n")

    if not results:
        print("No valid results. Exit.")
        return

    # è¾“å‡ºè¡¨æ ¼ï¼ˆä»resultsä¸­æå–metricsï¼Œæ’é™¤runnerå¯¹è±¡ï¼‰
    print("\n" + "=" * 80)
    print("Unified Metrics Comparison")
    print("=" * 80)
    print(f"{'Model':<28} | {'Feat MSE':>10} | {'Feat MAE':>10} | {'Cosine Sim':>11} | {'Edge Loss':>10} | {'FGD Loss':>9} | {'FSD Loss':>9}")
    print("-" * 80)
    for r in results:
        print(
            f"{r['name']:<28} | "
            f"{r['mse']:>10.6f} | "
            f"{r['mae']:>10.6f} | "
            f"{r['cosine_sim']:>11.6f} | "
            f"{r['edge_loss']:>10.6f} | "
            f"{r.get('fgd_loss', float('nan')):>9.6f} | "
            f"{r.get('fsd_loss', float('nan')):>9.6f}"
        )

    # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆåªä¿å­˜metricsï¼Œä¸åŒ…å«runnerå¯¹è±¡ï¼‰
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("Unified Metrics Comparison\n")
        f.write("=" * 80 + "\n")
        f.write(f"{'Model':<28} | {'Feat MSE':>10} | {'Feat MAE':>10} | {'Cosine Sim':>11} | {'Edge Loss':>10} | {'FGD Loss':>9} | {'FSD Loss':>9}\n")
        f.write("-" * 80 + "\n")
        for r in results:
            f.write(
                f"{r['name']:<28} | "
                f"{r['mse']:>10.6f} | "
                f"{r['mae']:>10.6f} | "
                f"{r['cosine_sim']:>11.6f} | "
                f"{r['edge_loss']:>10.6f} | "
                f"{r.get('fgd_loss', float('nan')):>9.6f} | "
                f"{r.get('fsd_loss', float('nan')):>9.6f}\n"
            )
    print(f"\nSaved to: {out_path}")
    
    # å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.visualize:
        print("\n" + "=" * 80)
        print("Generating Visualizations...")
        print("=" * 80)
        
        # åŠ è½½æµ‹è¯•æ•°æ®é›†
        test_dataset = NPZWithImageIdDataset(str(test_features_dir), str(test_images_dir), max_images=args.max_images, input_size=1024)
        num_vis = min(args.num_vis_samples, len(test_dataset))
        
        vis_output_dir = Path(args.vis_output_dir)
        vis_output_dir.mkdir(parents=True, exist_ok=True)
        
        if args.vis_mode == "comparison":
            visualize_comparison(results, test_dataset, vis_output_dir, num_vis)
        elif args.vis_mode == "separate":
            visualize_separate(results, test_dataset, vis_output_dir, num_vis)
        else:
            visualize_panel_4x4(results, test_dataset, vis_output_dir, num_vis)
        
        print(f"\nâœ… Visualizations saved to: {vis_output_dir}")
    
    print("=" * 80)


if __name__ == "__main__":
    main()


