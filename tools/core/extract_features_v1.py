#!/usr/bin/env python3
"""
ç¬¬ä¸€ç‰ˆç‰¹å¾æå–è„šæœ¬
ä½¿ç”¨SAM2.1hieraæ•™å¸ˆæ¨¡å‹æå–16xä¸‹é‡‡æ ·256é€šé“ç‰¹å¾
åŒæ—¶ä»SA-1B JSONç”Ÿæˆè¾¹ç¼˜å›¾(64x64å’Œ256x256)
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
import argparse
from tqdm import tqdm
import json
import cv2
from pycocotools import mask as mask_utils
import torch.nn.functional as F

# æ·»åŠ é¡¹ç›®è·¯å¾„å’ŒSAM2è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sam2_path = project_root / "vfmkd" / "sam2"
if str(sam2_path) not in sys.path:
    sys.path.insert(0, str(sam2_path))

from vfmkd.teachers.sam2_teacher import SAM2Teacher
from tools.core.bbox.test_bbox_strategies import (
    load_sa_json,
    compute_strategy_geometry_color,
)


class SA1BFeatureExtractor:
    """SA-1Bç‰¹å¾æå–å™¨"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºSAM2æ•™å¸ˆæ¨¡å‹
        self.teacher = SAM2Teacher(config['teacher'])
        
        # è¾¹ç¼˜æå–é…ç½®
        self.kernel_size = config.get('kernel_size', 3)
        self.kernel = np.ones((self.kernel_size, self.kernel_size), dtype=np.uint8)
        
        # é€‰æ¡†ç­–ç•¥é…ç½®                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             è¯´
        self.max_instances = config.get('max_instances', 1)
        self.enable_bbox_selection = config.get('enable_bbox_selection', True)
        
        print(f"âœ… ç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ•™å¸ˆæ¨¡å‹: {self.teacher.model_name}")
    
    def extract_edges_and_weights_optimized(self, json_path, edge_sizes=[256, 64, 32], weight_sizes=[128, 64, 32]):
        """
        ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨Method Bï¼ˆæ¯å®ä¾‹æå–è¾¹ç¼˜ååˆå¹¶ï¼‰+ åŒæ—¶ç”Ÿæˆè¾¹ç¼˜å›¾å’Œæƒé‡å›¾
        å®Œå…¨å¤åˆ»edge_comparisonä¸­çš„Method Bï¼ˆCPUä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            json_path: JSONæ ‡æ³¨æ–‡ä»¶è·¯å¾„
            edge_sizes: è¾¹ç¼˜å›¾ç›®æ ‡å°ºå¯¸åˆ—è¡¨
            weight_sizes: æƒé‡å›¾ç›®æ ‡å°ºå¯¸åˆ—è¡¨
            
        Returns:
            (edge_maps, weight_maps): è¾¹ç¼˜å›¾å­—å…¸å’Œæƒé‡å›¾å­—å…¸
        """
        # åŠ è½½JSONæ ‡æ³¨æ–‡ä»¶
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        # è·å–å›¾åƒå°ºå¯¸
        image_info = data['image']
        height = image_info['height']
        width = image_info['width']
        
        # è·å–æ‰€æœ‰RLEæ ‡æ³¨
        annotations = data['annotations']
        
        # === ä½¿ç”¨Method Bæå–è¾¹ç¼˜ï¼ˆCPUä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸edge_comparisonå®Œå…¨ä¸€è‡´ï¼‰===
        if len(annotations) == 0:
            # æ²¡æœ‰æ ‡æ³¨ï¼Œè¿”å›ç©ºå›¾
            union_mask = np.zeros((height, width), dtype=np.uint8)
            combined_edge_map = np.zeros((height, width), dtype=np.uint8)
        else:
            # Method Bï¼šæ¯ä¸ªå®ä¾‹å•ç‹¬æå–è¾¹ç¼˜ååˆå¹¶
            combined_edge_map = np.zeros((height, width), dtype=np.uint8)
            union_mask = np.zeros((height, width), dtype=np.uint8)
            
            for ann in annotations:
                rle = ann['segmentation']
                mask = mask_utils.decode(rle)  # ä»RLEè§£ç 
                
                # åˆå¹¶æ©ç ï¼ˆç”¨äºæƒé‡å›¾ï¼‰
                union_mask = np.maximum(union_mask, mask)
        
                # å¯¹æ¯ä¸ªå®ä¾‹å•ç‹¬æå–è¾¹ç¼˜
                edge = cv2.morphologyEx(mask, cv2.MORPH_GRADIENT, self.kernel)
                # äºŒå€¼åŒ–å¹¶ç¡®ä¿uint8ç±»å‹ï¼ˆé¿å…ç±»å‹ä¸åŒ¹é…å’Œæº¢å‡ºè­¦å‘Šï¼‰
                edge = (edge > 0).astype(np.uint8)
                
                # ä½¿ç”¨bitwise_oræ›¿ä»£logical_orï¼ˆç›´æ¥åœ¨uint8ä¸Šæ“ä½œï¼‰
                combined_edge_map = np.bitwise_or(combined_edge_map, edge)
        
        # === ç”Ÿæˆå¤šå°ºåº¦è¾¹ç¼˜å›¾ ===
        edge_maps = {'original': combined_edge_map}
        for size in edge_sizes:
            edge_float = combined_edge_map.astype(np.float32)
            edge_small = cv2.resize(edge_float, (size, size), interpolation=cv2.INTER_AREA)
            edge_maps[size] = (edge_small > 0).astype(np.uint8)
        
        # === ç”Ÿæˆæƒé‡å›¾ï¼ˆå¤ç”¨union_maskï¼‰ ===
        weight_maps = {}
        for size in weight_sizes:
            # ä¸‹é‡‡æ ·åˆ°ç‰¹å¾å›¾åˆ†è¾¨ç‡
            union_tensor = torch.from_numpy(union_mask.astype(np.float32)).unsqueeze(0).unsqueeze(0)
            fg_prob = F.adaptive_avg_pool2d(union_tensor, (size, size)).squeeze().numpy()
            
            # å‰æ™¯æƒé‡ï¼šareaå½’ä¸€åŒ–
            fg_binary = (fg_prob > 0.5).astype(np.float32)
            num_fg = np.clip(fg_binary.sum(), a_min=1, a_max=None)
            fg_map = fg_binary / num_fg
            
            # èƒŒæ™¯æƒé‡ï¼šå½’ä¸€åŒ–
            bg_map = 1.0 - fg_binary
            bg_sum = bg_map.sum()
            if bg_sum > 0:
                bg_map = bg_map / bg_sum
            
            weight_maps[size] = {
                'fg_map': fg_map.astype(np.float32),
                'bg_map': bg_map.astype(np.float32)
            }
        
        return edge_maps, weight_maps
    
    def process_single_image(self, image_path, json_path, output_dir):
        """
        å¤„ç†å•å¼ å›¾åƒï¼šæå–ç‰¹å¾å’Œè¾¹ç¼˜å›¾
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            json_path: JSONæ ‡æ³¨æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        import time
        image_id = Path(image_path).stem
        timing = {}  # è®°å½•å„æ­¥éª¤è€—æ—¶
        
        try:
            # 1. åŠ è½½å›¾åƒ
            t0 = time.time()
            image = cv2.imread(str(image_path))
            if image is None:
                raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {image_path}")
            
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            timing['load_image'] = time.time() - t0
            
            # 2. æå–SAM2ç‰¹å¾ï¼ˆSAM2 teacherå†…éƒ¨ä½¿ç”¨å®˜æ–¹transformï¼šresize + normalizeï¼‰
            t0 = time.time()
            features = self.teacher.extract_features(
                image_rgb, 
                image_ids=[image_id], 
                save_features=False  # æˆ‘ä»¬æ‰‹åŠ¨ä¿å­˜
            )
            timing['sam2_features'] = time.time() - t0
            
            # 3. åŒæ—¶æå–è¾¹ç¼˜å›¾å’Œæƒé‡å›¾ï¼ˆä¼˜åŒ–ç‰ˆï¼šåˆå¹¶è§£ç ï¼Œåªåšä¸€æ¬¡å½¢æ€å­¦ï¼‰
            t0 = time.time()
            edge_maps, weight_maps = self.extract_edges_and_weights_optimized(
                json_path, 
                edge_sizes=[256, 64, 32],  # ç§»é™¤128ä»¥åŠ é€Ÿ
                weight_sizes=[128, 64, 32]
            )
            timing['edges_and_weights'] = time.time() - t0
            
            # 4. åº”ç”¨geometry_colorç­–ç•¥é€‰æ‹©æ¡†å’Œæ©ç ï¼ˆæ–°å¢ï¼Œå¤ç”¨å·²æœ‰image_rgbï¼‰
            bbox_result = None
            if self.enable_bbox_selection:
                t0 = time.time()
                try:
                    # å‡†å¤‡å›¾åƒä¿¡æ¯ï¼ˆå¤ç”¨å·²æœ‰çš„image_rgbï¼Œé¿å…é‡å¤è¯»å–ï¼‰
                    H, W = image_rgb.shape[:2]
                    data = {
                        'image': {
                            'height': H,
                            'width': W,
                            'h': H,
                            'w': W,
                        }
                    }
                    
                    # åŠ è½½JSONæ ‡æ³¨ï¼ˆå¤ç”¨json_pathï¼‰
                    sa_data = load_sa_json(str(json_path))
                    annotations = sa_data.get('annotations', [])
                    
                    # åº”ç”¨ç­–ç•¥ï¼ˆç›´æ¥ä½¿ç”¨å·²æœ‰çš„image_rgbï¼Œæ³¨æ„éœ€è¦BGRæ ¼å¼ï¼‰
                    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
                    selected_components = compute_strategy_geometry_color(
                        data=data,
                        annotations=annotations,
                        image_rgb=image_bgr,  # å‡½æ•°å†…éƒ¨ä¼šè½¬æ¢ï¼Œè¿™é‡Œä¼ å…¥BGR
                        clip_data=None,
                        max_instances=self.max_instances,
                        max_display=10,
                        debug_trace=None,
                    )
                    
                    # æå–æ¡†å’Œæ©ç 
                    if len(selected_components) > 0:
                        bboxes = []
                        masks = []
                        for comp in selected_components:
                            bboxes.append(comp['box'])  # [x, y, w, h]
                            masks.append(comp['mask'])  # [H, W] uint8
                        
                        bbox_result = {
                            'has_bbox': True,
                            'bboxes': np.array(bboxes, dtype=np.float32),
                            'masks': masks,
                        }
                    else:
                        bbox_result = {
                            'has_bbox': False,
                            'bboxes': np.empty((0, 4), dtype=np.float32),
                            'masks': [],
                        }
                    timing['bbox_selection'] = time.time() - t0
                except Exception as e:
                    # é€‰æ¡†å¤±è´¥ä¸å½±å“å…¶ä»–æ•°æ®ä¿å­˜ï¼Œä½¿ç”¨ç©ºç»“æœ
                    print(f"âš ï¸  é€‰æ¡†ç­–ç•¥å¤±è´¥ {image_id}: {e}")
                    bbox_result = {
                        'has_bbox': False,
                        'bboxes': np.empty((0, 4), dtype=np.float32),
                        'masks': [],
                    }
                    timing['bbox_selection'] = 0.0
            else:
                bbox_result = None
                timing['bbox_selection'] = 0.0
            
            # 5. ä¿å­˜ç‰¹å¾ã€è¾¹ç¼˜å›¾å’Œæƒé‡å›¾
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜NPZæ–‡ä»¶ï¼ˆç‰¹å¾ + è¾¹ç¼˜å›¾ + æƒé‡å›¾ï¼‰
            save_data = {}
            
            # æ·»åŠ SAM2ç‰¹å¾
            for key, feat in features.items():
                save_data[key] = feat.detach().cpu().numpy()
            
            # æ·»åŠ è¾¹ç¼˜å›¾ï¼ˆå¤šå°ºåº¦ï¼Œç§»é™¤edge_originalä»¥èŠ‚çœç©ºé—´ï¼‰
            # save_data['edge_original'] = edge_maps['original']  # åŸå›¾å°ºå¯¸è¾¹ç¼˜å›¾ï¼ˆå·²ç§»é™¤ä»¥èŠ‚çœç©ºé—´ï¼‰
            save_data['edge_256x256'] = edge_maps[256]         # 256x256è¾¹ç¼˜å›¾
            save_data['edge_64x64'] = edge_maps[64]            # 64x64è¾¹ç¼˜å›¾ (å¯¹åº”s16, P4)
            save_data['edge_32x32'] = edge_maps[32]            # 32x32è¾¹ç¼˜å›¾ (å¯¹åº”s32, P5)
            
            # æ·»åŠ å‰æ™¯/èƒŒæ™¯æƒé‡å›¾ï¼ˆå¤šå°ºåº¦ï¼‰
            for size in [128, 64, 32]:
                save_data[f'fg_map_{size}x{size}'] = weight_maps[size]['fg_map']
                save_data[f'bg_map_{size}x{size}'] = weight_maps[size]['bg_map']
            
            # æ·»åŠ é€‰æ¡†ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if bbox_result is not None:
                save_data['has_bbox'] = np.array(bbox_result['has_bbox'], dtype=bool)
                save_data['num_bboxes'] = np.array(len(bbox_result['bboxes']), dtype=np.int32)
                save_data['bboxes'] = bbox_result['bboxes']  # (N, 4) or (0, 4)
                if bbox_result['has_bbox'] and len(bbox_result['masks']) > 0:
                    save_data['masks'] = np.array(bbox_result['masks'], dtype=object)
                save_data['geometry_color_flag'] = np.array(1, dtype=np.uint8)  # æ ‡è®°å·²å¤„ç†
            
            # æ·»åŠ å…ƒæ•°æ®
            save_data['image_id'] = image_id
            save_data['image_shape'] = np.array(image_rgb.shape)
            save_data['model_type'] = self.teacher.model_type
            
            # ä¿å­˜NPZæ–‡ä»¶
            t0 = time.time()
            npz_file = output_path / f"{image_id}_features.npz"
            np.savez(npz_file, **save_data)
            timing['save_npz'] = time.time() - t0
            
            # è®¡ç®—æ€»æ—¶é—´
            total_time = sum(timing.values())
            
            return {
                'success': True,
                'image_id': image_id,
                'npz_file': npz_file,
                'feature_shape': features['P4_S16'].shape,  # ä½¿ç”¨P4_S16æ›¿ä»£IMAGE_EMB_S16
                'edge_shapes': {f'edge_{k}': v.shape for k, v in edge_maps.items()},
                'timing': timing,
                'total_time': total_time
            }
            
        except Exception as e:
            return {
                'success': False,
                'image_id': image_id,
                'error': str(e)
            }
    
    def batch_extract_features(self, data_dir, output_dir, max_images=None):
        """
        æ‰¹é‡æå–ç‰¹å¾å’Œè¾¹ç¼˜å›¾
        
        Args:
            data_dir: SA-1Bæ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            max_images: æœ€å¤§å¤„ç†å›¾åƒæ•°é‡
        """
        data_path = Path(data_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = list(data_path.glob("*.jpg"))
        if max_images:
            image_files = image_files[:max_images]
        
        # æ£€æŸ¥å·²å­˜åœ¨çš„NPZæ–‡ä»¶ï¼ˆå»é‡ï¼‰
        existing_npz = set()
        for npz_file in output_path.glob("*_features.npz"):
            image_id = npz_file.stem.replace('_features', '')
            existing_npz.add(image_id)
        
        print(f"ğŸ“ æ€»å›¾åƒæ•°: {len(image_files)}")
        print(f"âœ… å·²æå–: {len(existing_npz)} ä¸ª")
        print(f"â³ å¾…å¤„ç†: {len(image_files) - len(existing_npz)} ä¸ª")
        
        success_count = 0
        error_count = 0
        skipped_count = 0
        avg_timing = {'load_image': 0, 'sam2_features': 0, 'edges_and_weights': 0, 
                      'bbox_selection': 0, 'save_npz': 0}
        
        for image_file in tqdm(image_files, desc="æå–ç‰¹å¾å’Œè¾¹ç¼˜å›¾"):
            image_id = image_file.stem
            
            # å»é‡æ£€æŸ¥ï¼šå¦‚æœNPZæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
            if image_id in existing_npz:
                skipped_count += 1
                continue
            
            # æŸ¥æ‰¾å¯¹åº”çš„JSONæ–‡ä»¶
            json_file = data_path / f"{image_file.stem}.json"
            if not json_file.exists():
                print(f"âš ï¸  è·³è¿‡ {image_file.stem}: æ‰¾ä¸åˆ°å¯¹åº”çš„JSONæ–‡ä»¶")
                error_count += 1
                continue
            
            # å¤„ç†å›¾åƒ
            result = self.process_single_image(image_file, json_file, output_path)
            
            if result['success']:
                success_count += 1
                # ç´¯åŠ è®¡æ—¶ç»Ÿè®¡
                for key in avg_timing:
                    if key in result.get('timing', {}):
                        avg_timing[key] += result['timing'][key]
                
                # æ¯10å¼ æ‰“å°ä¸€æ¬¡è¯¦ç»†è®¡æ—¶
                if success_count % 10 == 0:
                    bbox_time = result['timing'].get('bbox_selection', 0)
                    print(f"\nâœ… {result['image_id']}: æ€»{result['total_time']:.2f}s "
                          f"[åŠ è½½{result['timing']['load_image']:.3f}s | "
                          f"SAM2ç‰¹å¾{result['timing']['sam2_features']:.3f}s | "
                          f"è¾¹ç¼˜+æƒé‡å›¾{result['timing']['edges_and_weights']:.3f}s | "
                          f"é€‰æ¡†{bbox_time:.3f}s | "
                          f"ä¿å­˜{result['timing']['save_npz']:.3f}s]")
            else:
                error_count += 1
                print(f"âŒ {result['image_id']}: {result['error']}")
        
        print(f"\nğŸ‰ ç‰¹å¾æå–å®Œæˆ!")
        print(f"âœ… æœ¬æ¬¡æˆåŠŸ: {success_count} ä¸ª")
        print(f"â­ï¸  å·²è·³è¿‡(å»é‡): {skipped_count} ä¸ª")
        print(f"âŒ å¤±è´¥: {error_count} ä¸ª")
        print(f"ğŸ“Š æ€»è®¡å·²æå–: {len(existing_npz) + success_count} ä¸ª")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
        
        # æ‰“å°å¹³å‡è€—æ—¶ç»Ÿè®¡
        if success_count > 0:
            print(f"\nâ±ï¸  å¹³å‡è€—æ—¶ (æ¯å¼ ):")
            for key, total_time in avg_timing.items():
                avg_time = total_time / success_count
                print(f"  {key}: {avg_time:.3f}s")
            total_avg = sum(avg_timing.values()) / success_count
            print(f"  æ€»è®¡: {total_avg:.3f}s")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç¬¬ä¸€ç‰ˆç‰¹å¾æå–è„šæœ¬")
    parser.add_argument("--data-dir", type=str, required=True, help="SA-1Bæ•°æ®ç›®å½•")
    parser.add_argument("--output-dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max-images", type=int, default=None, help="æœ€å¤§å¤„ç†å›¾åƒæ•°é‡")
    parser.add_argument("--teacher-model", type=str, default="sam2.1_hiera_b+", 
                       choices=["sam2.1_hiera_t", "sam2.1_hiera_s", "sam2.1_hiera_b+", "sam2.1_hiera_l"],
                       help="æ•™å¸ˆæ¨¡å‹ç±»å‹")
    parser.add_argument("--checkpoint", type=str, default=None, help="æƒé‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--kernel-size", type=int, default=3, help="è¾¹ç¼˜æå–æ ¸å¤§å°")
    parser.add_argument("--device", type=str, default="cuda:6", help="æŒ‡å®šGPUè®¾å¤‡ (å¦‚: cuda:0, cuda:3)ï¼Œé»˜è®¤cuda:6")
    parser.add_argument("--diag-compare", action='store_true', help="å¯ç”¨è¯Šæ–­ï¼šä¿å­˜å‰å¯¹æ¯”åˆ†å¸ƒï¼Œæ‰“å°mean/std")
    parser.add_argument("--diag-fallback", action='store_true', help="å¯ç”¨å›é€€ï¼šè‹¥stdå¼‚å¸¸åˆ™å›é€€ä¸º/255å®æ—¶ç‰¹å¾")
    parser.add_argument('--max-instances', type=int, default=1,
                       help='é€‰æ¡†ç­–ç•¥æœ€å¤šé€‰æ‹©çš„å®ä¾‹æ•°ï¼ˆé»˜è®¤1ï¼‰')
    parser.add_argument('--enable-bbox-selection', action='store_true', default=True,
                       help='å¯ç”¨é€‰æ¡†ç­–ç•¥ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--disable-bbox-selection', dest='enable_bbox_selection', action='store_false',
                       help='ç¦ç”¨é€‰æ¡†ç­–ç•¥')
    
    args = parser.parse_args()
    
    # æ„å»ºé…ç½®
    # ç»Ÿä¸€æƒé‡è·¯å¾„ï¼šb+ å¯¹åº” base_plus æƒé‡æ–‡ä»¶å
    _ckpt = args.checkpoint
    if _ckpt is None:
        if args.teacher_model.endswith('hiera_b+'):
            _ckpt = 'weights/sam2.1_hiera_base_plus.pt'
        else:
            _ckpt = 'weights/sam2.1_hiera_base_plus.pt'
    
    # ç¡®å®šä½¿ç”¨çš„è®¾å¤‡
    if args.device:
        device = args.device
    else:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'

    config = {
        'teacher': {
            'model_type': args.teacher_model,
            'checkpoint_path': _ckpt,
            'device': device,
            'enable_visualization': False,  # å…³é—­å¯è§†åŒ–ä»¥æé«˜é€Ÿåº¦
            'feature_output_dir': args.output_dir,
            'enable_diag_compare': bool(args.diag_compare),
            'fallback_if_high_std': bool(args.diag_fallback),
        },
        'kernel_size': args.kernel_size,
        'max_instances': args.max_instances,
        'enable_bbox_selection': args.enable_bbox_selection,
    }
    
    print("=== ç¬¬ä¸€ç‰ˆç‰¹å¾æå– ===")
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"æ•™å¸ˆæ¨¡å‹: {args.teacher_model}")
    print(f"æƒé‡æ–‡ä»¶: {config['teacher']['checkpoint_path']}")
    print(f"è®¡ç®—è®¾å¤‡: {device}")
    print(f"æœ€å¤§å›¾åƒæ•°: {args.max_images or 'å…¨éƒ¨'}")
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = SA1BFeatureExtractor(config)
    
    # æ‰¹é‡æå–ç‰¹å¾
    extractor.batch_extract_features(
        args.data_dir,
        args.output_dir,
        args.max_images
    )


if __name__ == "__main__":
    main()
