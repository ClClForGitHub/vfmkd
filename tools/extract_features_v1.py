#!/usr/bin/env python3
"""
ç¬¬ä¸€ç‰ˆç‰¹å¾æå–è„šæœ¬
ä½¿ç”¨SAM2.1hieraæ•™å¸ˆæ¨¡å‹æå–16xä¸‹é‡‡æ ·256é€šé“ç‰¹å¾
åŒæ—¶ä»SA-1B JSONç”Ÿæˆè¾¹ç¼˜å›¾(64x64å’Œ256x256)
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
import argparse
from tqdm import tqdm
import json
import cv2
from pycocotools import mask as mask_utils
import torch.nn.functional as F

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from vfmkd.teachers.sam2_teacher import SAM2Teacher


class SA1BFeatureExtractor:
    """SA-1Bç‰¹å¾æå–å™¨"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºSAM2æ•™å¸ˆæ¨¡å‹
        self.teacher = SAM2Teacher(config['teacher'])
        
        # è¾¹ç¼˜æå–é…ç½®
        self.kernel_size = config.get('kernel_size', 3)
        self.kernel = np.ones((self.kernel_size, self.kernel_size), dtype=np.uint8)
        
        print(f"âœ… ç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ•™å¸ˆæ¨¡å‹: {self.teacher.model_name}")
    
    def extract_edges_and_weights_optimized(self, json_path, edge_sizes=[256, 64, 32], weight_sizes=[128, 64, 32]):
        """
        ä¼˜åŒ–ç‰ˆï¼šåˆå¹¶è§£ç  + åªåšä¸€æ¬¡å½¢æ€å­¦æ“ä½œ + åŒæ—¶ç”Ÿæˆè¾¹ç¼˜å›¾å’Œæƒé‡å›¾
        
        Args:
            json_path: JSONæ ‡æ³¨æ–‡ä»¶è·¯å¾„
            edge_sizes: è¾¹ç¼˜å›¾ç›®æ ‡å°ºå¯¸åˆ—è¡¨
            weight_sizes: æƒé‡å›¾ç›®æ ‡å°ºå¯¸åˆ—è¡¨
            
        Returns:
            (edge_maps, weight_maps): è¾¹ç¼˜å›¾å­—å…¸å’Œæƒé‡å›¾å­—å…¸
        """
        # åŠ è½½JSONæ ‡æ³¨æ–‡ä»¶
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        # è·å–å›¾åƒå°ºå¯¸
        image_info = data['image']
        height = image_info['height']
        width = image_info['width']
        
        # è·å–æ‰€æœ‰RLEæ ‡æ³¨
        annotations = data['annotations']
        
        if len(annotations) == 0:
            # æ²¡æœ‰æ ‡æ³¨ï¼Œè¿”å›ç©ºå›¾
            union_mask = np.zeros((height, width), dtype=np.uint8)
        else:
            # å•è¿›ç¨‹é¡ºåºè§£ç å¹¶åˆå¹¶ï¼ˆé¿å…Windowså¤šè¿›ç¨‹å¼€é”€ï¼‰
            union_mask = np.zeros((height, width), dtype=np.uint8)
            for ann in annotations:
                rle = ann['segmentation']
                mask = mask_utils.decode(rle)
                union_mask = np.maximum(union_mask, mask)
        
        # åœ¨åˆå¹¶åçš„æ©ç ä¸Šæå–è¾¹ç¼˜ï¼ˆåªåšä¸€æ¬¡å½¢æ€å­¦æ“ä½œï¼ï¼‰
        combined_edge_map = cv2.morphologyEx(union_mask, cv2.MORPH_GRADIENT, self.kernel)
        
        # === ç”Ÿæˆå¤šå°ºåº¦è¾¹ç¼˜å›¾ ===
        edge_maps = {'original': combined_edge_map}
        for size in edge_sizes:
            edge_float = combined_edge_map.astype(np.float32)
            edge_small = cv2.resize(edge_float, (size, size), interpolation=cv2.INTER_AREA)
            edge_maps[size] = (edge_small > 0).astype(np.uint8)
        
        # === ç”Ÿæˆæƒé‡å›¾ï¼ˆå¤ç”¨union_maskï¼‰ ===
        weight_maps = {}
        for size in weight_sizes:
            # ä¸‹é‡‡æ ·åˆ°ç‰¹å¾å›¾åˆ†è¾¨ç‡
            union_tensor = torch.from_numpy(union_mask.astype(np.float32)).unsqueeze(0).unsqueeze(0)
            fg_prob = F.adaptive_avg_pool2d(union_tensor, (size, size)).squeeze().numpy()
            
            # å‰æ™¯æƒé‡ï¼šareaå½’ä¸€åŒ–
            fg_binary = (fg_prob > 0.5).astype(np.float32)
            num_fg = np.clip(fg_binary.sum(), a_min=1, a_max=None)
            fg_map = fg_binary / num_fg
            
            # èƒŒæ™¯æƒé‡ï¼šå½’ä¸€åŒ–
            bg_map = 1.0 - fg_binary
            bg_sum = bg_map.sum()
            if bg_sum > 0:
                bg_map = bg_map / bg_sum
            
            weight_maps[size] = {
                'fg_map': fg_map.astype(np.float32),
                'bg_map': bg_map.astype(np.float32)
            }
        
        return edge_maps, weight_maps
    
    def process_single_image(self, image_path, json_path, output_dir):
        """
        å¤„ç†å•å¼ å›¾åƒï¼šæå–ç‰¹å¾å’Œè¾¹ç¼˜å›¾
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            json_path: JSONæ ‡æ³¨æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        import time
        image_id = Path(image_path).stem
        timing = {}  # è®°å½•å„æ­¥éª¤è€—æ—¶
        
        try:
            # 1. åŠ è½½å›¾åƒ
            t0 = time.time()
            image = cv2.imread(str(image_path))
            if image is None:
                raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {image_path}")
            
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            timing['load_image'] = time.time() - t0
            
            # 2. æå–SAM2ç‰¹å¾ï¼ˆSAM2 teacherå†…éƒ¨ä½¿ç”¨å®˜æ–¹transformï¼šresize + normalizeï¼‰
            t0 = time.time()
            features = self.teacher.extract_features(
                image_rgb, 
                image_ids=[image_id], 
                save_features=False  # æˆ‘ä»¬æ‰‹åŠ¨ä¿å­˜
            )
            timing['sam2_features'] = time.time() - t0
            
            # 3. åŒæ—¶æå–è¾¹ç¼˜å›¾å’Œæƒé‡å›¾ï¼ˆä¼˜åŒ–ç‰ˆï¼šåˆå¹¶è§£ç ï¼Œåªåšä¸€æ¬¡å½¢æ€å­¦ï¼‰
            t0 = time.time()
            edge_maps, weight_maps = self.extract_edges_and_weights_optimized(
                json_path, 
                edge_sizes=[256, 64, 32],  # ç§»é™¤128ä»¥åŠ é€Ÿ
                weight_sizes=[128, 64, 32]
            )
            timing['edges_and_weights'] = time.time() - t0
            
            # 5. ä¿å­˜ç‰¹å¾ã€è¾¹ç¼˜å›¾å’Œæƒé‡å›¾
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜NPZæ–‡ä»¶ï¼ˆç‰¹å¾ + è¾¹ç¼˜å›¾ + æƒé‡å›¾ï¼‰
            save_data = {}
            
            # æ·»åŠ SAM2ç‰¹å¾
            for key, feat in features.items():
                save_data[key] = feat.detach().cpu().numpy()
            
            # æ·»åŠ è¾¹ç¼˜å›¾ï¼ˆå¤šå°ºåº¦ï¼Œç§»é™¤128x128ä»¥åŠ é€Ÿï¼‰
            save_data['edge_original'] = edge_maps['original']  # åŸå›¾å°ºå¯¸è¾¹ç¼˜å›¾
            save_data['edge_256x256'] = edge_maps[256]         # 256x256è¾¹ç¼˜å›¾
            save_data['edge_64x64'] = edge_maps[64]            # 64x64è¾¹ç¼˜å›¾ (å¯¹åº”s16, P4)
            save_data['edge_32x32'] = edge_maps[32]            # 32x32è¾¹ç¼˜å›¾ (å¯¹åº”s32, P5)
            
            # æ·»åŠ å‰æ™¯/èƒŒæ™¯æƒé‡å›¾ï¼ˆå¤šå°ºåº¦ï¼‰
            for size in [128, 64, 32]:
                save_data[f'fg_map_{size}x{size}'] = weight_maps[size]['fg_map']
                save_data[f'bg_map_{size}x{size}'] = weight_maps[size]['bg_map']
            
            # æ·»åŠ å…ƒæ•°æ®
            save_data['image_id'] = image_id
            save_data['image_shape'] = np.array(image_rgb.shape)
            save_data['model_type'] = self.teacher.model_type
            
            # ä¿å­˜NPZæ–‡ä»¶
            t0 = time.time()
            npz_file = output_path / f"{image_id}_features.npz"
            np.savez(npz_file, **save_data)
            timing['save_npz'] = time.time() - t0
            
            # è®¡ç®—æ€»æ—¶é—´
            total_time = sum(timing.values())
            
            return {
                'success': True,
                'image_id': image_id,
                'npz_file': npz_file,
                'feature_shape': features['P4_S16'].shape,  # ä½¿ç”¨P4_S16æ›¿ä»£IMAGE_EMB_S16
                'edge_shapes': {f'edge_{k}': v.shape for k, v in edge_maps.items()},
                'timing': timing,
                'total_time': total_time
            }
            
        except Exception as e:
            return {
                'success': False,
                'image_id': image_id,
                'error': str(e)
            }
    
    def batch_extract_features(self, data_dir, output_dir, max_images=None):
        """
        æ‰¹é‡æå–ç‰¹å¾å’Œè¾¹ç¼˜å›¾
        
        Args:
            data_dir: SA-1Bæ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            max_images: æœ€å¤§å¤„ç†å›¾åƒæ•°é‡
        """
        data_path = Path(data_dir)
        output_path = Path(output_dir)
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = list(data_path.glob("*.jpg"))
        if max_images:
            image_files = image_files[:max_images]
        
        print(f"å¼€å§‹å¤„ç† {len(image_files)} ä¸ªå›¾åƒ...")
        
        success_count = 0
        error_count = 0
        avg_timing = {'load_image': 0, 'sam2_features': 0, 'edges_and_weights': 0, 'save_npz': 0}
        
        for image_file in tqdm(image_files, desc="æå–ç‰¹å¾å’Œè¾¹ç¼˜å›¾"):
            # æŸ¥æ‰¾å¯¹åº”çš„JSONæ–‡ä»¶
            json_file = data_path / f"{image_file.stem}.json"
            if not json_file.exists():
                print(f"âš ï¸  è·³è¿‡ {image_file.stem}: æ‰¾ä¸åˆ°å¯¹åº”çš„JSONæ–‡ä»¶")
                error_count += 1
                continue
            
            # å¤„ç†å›¾åƒ
            result = self.process_single_image(image_file, json_file, output_path)
            
            if result['success']:
                success_count += 1
                # ç´¯åŠ è®¡æ—¶ç»Ÿè®¡
                for key in avg_timing:
                    if key in result.get('timing', {}):
                        avg_timing[key] += result['timing'][key]
                
                # æ¯10å¼ æ‰“å°ä¸€æ¬¡è¯¦ç»†è®¡æ—¶
                if success_count % 10 == 0:
                    print(f"\nâœ… {result['image_id']}: æ€»{result['total_time']:.2f}s "
                          f"[åŠ è½½{result['timing']['load_image']:.3f}s | "
                          f"SAM2ç‰¹å¾{result['timing']['sam2_features']:.3f}s | "
                          f"è¾¹ç¼˜+æƒé‡å›¾{result['timing']['edges_and_weights']:.3f}s | "
                          f"ä¿å­˜{result['timing']['save_npz']:.3f}s]")
            else:
                error_count += 1
                print(f"âŒ {result['image_id']}: {result['error']}")
        
        print(f"\nğŸ‰ ç‰¹å¾æå–å®Œæˆ!")
        print(f"æˆåŠŸ: {success_count} ä¸ª")
        print(f"å¤±è´¥: {error_count} ä¸ª")
        print(f"è¾“å‡ºç›®å½•: {output_path}")
        
        # æ‰“å°å¹³å‡è€—æ—¶ç»Ÿè®¡
        if success_count > 0:
            print(f"\nâ±ï¸  å¹³å‡è€—æ—¶ (æ¯å¼ ):")
            for key, total_time in avg_timing.items():
                avg_time = total_time / success_count
                print(f"  {key}: {avg_time:.3f}s")
            total_avg = sum(avg_timing.values()) / success_count
            print(f"  æ€»è®¡: {total_avg:.3f}s")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç¬¬ä¸€ç‰ˆç‰¹å¾æå–è„šæœ¬")
    parser.add_argument("--data-dir", type=str, required=True, help="SA-1Bæ•°æ®ç›®å½•")
    parser.add_argument("--output-dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max-images", type=int, default=None, help="æœ€å¤§å¤„ç†å›¾åƒæ•°é‡")
    parser.add_argument("--teacher-model", type=str, default="sam2.1_hiera_b+", 
                       choices=["sam2.1_hiera_t", "sam2.1_hiera_s", "sam2.1_hiera_b+", "sam2.1_hiera_l"],
                       help="æ•™å¸ˆæ¨¡å‹ç±»å‹")
    parser.add_argument("--checkpoint", type=str, default=None, help="æƒé‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--kernel-size", type=int, default=3, help="è¾¹ç¼˜æå–æ ¸å¤§å°")
    parser.add_argument("--diag-compare", action='store_true', help="å¯ç”¨è¯Šæ–­ï¼šä¿å­˜å‰å¯¹æ¯”åˆ†å¸ƒï¼Œæ‰“å°mean/std")
    parser.add_argument("--diag-fallback", action='store_true', help="å¯ç”¨å›é€€ï¼šè‹¥stdå¼‚å¸¸åˆ™å›é€€ä¸º/255å®æ—¶ç‰¹å¾")
    
    args = parser.parse_args()
    
    # æ„å»ºé…ç½®
    # ç»Ÿä¸€æƒé‡è·¯å¾„ï¼šb+ å¯¹åº” base_plus æƒé‡æ–‡ä»¶å
    _ckpt = args.checkpoint
    if _ckpt is None:
        if args.teacher_model.endswith('hiera_b+'):
            _ckpt = 'weights/sam2.1_hiera_base_plus.pt'
        else:
            _ckpt = 'weights/sam2.1_hiera_base_plus.pt'

    config = {
        'teacher': {
            'model_type': args.teacher_model,
            'checkpoint_path': _ckpt,
            'device': 'cuda' if torch.cuda.is_available() else 'cpu',
            'enable_visualization': False,  # å…³é—­å¯è§†åŒ–ä»¥æé«˜é€Ÿåº¦
            'feature_output_dir': args.output_dir,
            'enable_diag_compare': bool(args.diag_compare),
            'fallback_if_high_std': bool(args.diag_fallback),
        },
        'kernel_size': args.kernel_size
    }
    
    print("=== ç¬¬ä¸€ç‰ˆç‰¹å¾æå– ===")
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"æ•™å¸ˆæ¨¡å‹: {args.teacher_model}")
    print(f"æƒé‡æ–‡ä»¶: {config['teacher']['checkpoint_path']}")
    print(f"æœ€å¤§å›¾åƒæ•°: {args.max_images or 'å…¨éƒ¨'}")
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = SA1BFeatureExtractor(config)
    
    # æ‰¹é‡æå–ç‰¹å¾
    extractor.batch_extract_features(
        args.data_dir,
        args.output_dir,
        args.max_images
    )


if __name__ == "__main__":
    main()
